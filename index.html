<!DOCTYPE html>
<html class="js no-flexbox canvas canvastext webgl no-touch geolocation postmessage websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent video audio localstorage sessionstorage webworkers applicationcache svg inlinesvg smil svgclippaths gr_weikaichen_github_io" lang="en"><!--<![endif]-->
<head>
	<!-- Google tag (gtag.js) -->

	<style>
  .custom-list li {
    margin-bottom: 5.2px;
    margin-right: 3px;
  }
  .li_data {
    display: inline-block;
    width: 70px;
    font-weight: bold;
	      vertical-align: top;
  }
  .li_content {
    display: inline-block;
    width: calc(100% - 70px);
	      vertical-align: top;
  }
  .li_data_short {
    display: inline-block;
    width: 40px;
    font-weight: bold;
	      vertical-align: top;
  }
  .li_content_short {
    display: inline-block;
    width: calc(100% - 40px);
	      vertical-align: top;
  }

        .half-br {
            display: block;
            height: 50vh; /* 50% of the viewport height */
            content: "";
        }

</style>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-PYC1GJ7PZQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-PYC1GJ7PZQ');
</script>

	<style>
		.bio_container {
      /*display: flex;*/
			/*flex-direction: row;*/
			/*align-items: flex-start; !* ÂõæÁâáÂíåÊñáÂ≠óÈ°∂ÈÉ®ÂØπÈΩê *!*/

      /*align-items: top;*/
    }
    .slider-container {
      width: 213px;
		height: 220px;
      overflow: hidden;
      position: relative;
		margin: 3px;
		/*flex-shrink: 0;*/
		/*  flex: 0 0 auto; !* ÂõæÁâáÂÆπÂô®‰∏ç‰ºö‰º∏Áº© *!*/
		float: left;

    }
	 .slider-text {
		 /*padding: 10px;*/
		 /*margin: 10px;*/
      /*position: relative;*/
		/*margin: 30px;*/
		/* margin-left: 10px;*/
		   /*float: left; !* ÊñáÂ≠óÂÆπÂô®ÂêëÂ∑¶ÊµÆÂä® *!*/
		 /*width: 1000px;*/
		 /*height: 220px;*/
		 /*height: 100%;*/
		 /*  flex: 1 1 auto; !* ÊñáÂ≠óÂÆπÂô®‰º∏Áº©Â°´ÂÖÖÂâ©‰ΩôÁ©∫Èó¥ *!*/

		            /*float: left;*/
    }
    .slider {
      	display: flex;
		width: 220px;
				height: 210px;
      transition: transform 0.5s ease-in-out;
    }

    .slide {
		width: 220px;
		/*control the window size*/
		height: 210px;
      flex-shrink: 0;
    }

    .slide img {
      /*width: 98.5%;*/
      /*height: 99%;*/
		width: 200px;
		height: 210px;

    }

    .slider-controls {
      position: absolute;
      bottom: 10px;
      left: 50%;
      transform: translateX(-50%);
      display: flex;
    }

    .slider-controls button {
      background: none;
      border: none;
      cursor: pointer;
      padding: 10px;
      margin: 0 60px;
		color: darkgrey;
		font-weight: bold;
		font-size: 20px;

    }
	.custom-line-height {
      line-height: 10em; /* 0.5 times the font size */
    }
  </style>
	 <style>
    /* Ê∑ªÂä† CSS ËøáÊ∏°ÊïàÊûú */
    body {
      scroll-behavior: smooth;
    }
	#vis_rl {
      font-weight: normal;
      transition: font-weight 1s ease; /* Ê∑ªÂä†ËøáÊ∏°ÊïàÊûú */
    }

    /* Èó™ÁÉÅÊïàÊûú */
    @keyframes highlight {
      0% { background-color: yellow; }
      50% { background-color: transparent; }
      100% { background-color: yellow; }
    }

    .highlighted {
      animation: highlight 3s infinite; /* 3ÁßíÁöÑÈó™ÁÉÅÊïàÊûúÔºåinfinite Ë°®Á§∫Êó†ÈôêÂæ™ÁéØ */
    }
  </style>




	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


	<title>Zhiyang (Frank) Dou</title>

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<script src="./files/modernizr-2.5.3.min.js.download"></script>

	<script src="./files/jquery.min.js.download"></script>
	<script>window.jQuery || document.write('<script src="/js/vendor/jquery-1.7.2.min.js"><\/script>')</script>






	<script src="./files/spamspan.min.js.download"></script>
	<script src="./files/prettify.js.download"></script>

	<link rel="stylesheet" href="./files/social_widget.css">
	<link rel="stylesheet" href="./files/glyphicons.css">
	<link rel="stylesheet" href="./files/bootstrap.css">
	<link rel="stylesheet" href="./files/bootstrap-responsive.css">
	<link rel="stylesheet" href="./files/app.css">
	<!-- <link rel="stylesheet" href="./files/font-awesome.min.css"> -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<script type="text/javascript" src="./files/plugins.js.download"></script>
	<script type="text/javascript" src="./files/main.js.download"></script>
	<link rel="canonical" href="https://frank-zy-dou.github.io/">

	<!-- to toggle text -->
	<style type="text/css">
		a.toggle_text_link {
			cursor:pointer;
		}

		pre.invisible_text {
			display: none;
		}
	</style>
	<script language="javascript" type="text/javascript">
		function toggle(element) {
			if(element.style.display=="block") {
				element.style.display="none";
			} else {
				element.style.display="block";
			}
		}
	</script>

</head>

<body class="home page page-id-4 page-template-default top-navbar" data-gr-c-s-loaded="true">
  <!--[if lt IE 7]><div class="alert">Your browser is <em>ancient!</em> <a href="http://browsehappy.com/">Upgrade to a different browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to experience this site.</div><![endif]-->

    <header id="banner" class="navbar navbar-fixed-top" role="banner">
		<div class="navbar-inner">
			<div class="container">
				<a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
					<!---<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				-->
				</a>
				<a class="brand" href="https://frank-zy-dou.github.io/">
					Zhiyang (Frank) Dou
				</a>
				<nav id="nav-main" class="nav-collapse" role="navigation">
					<ul class="nav">
						<li class="menu-home active active"><a href="https://frank-zy-dou.github.io/">Home</a></li>
						<li class="menu-publications"><a href="./publication.html">Publications</a></li>
<!--						<li class="menu-publications"><a href="./research_statement.html">Research Statement</a></li>-->
						<!---<li class="menu-cv"><a href="http://herohuyongtao.github.io/cv/">CV</a></li>-->
					</ul>
				</nav>
			</div>
		</div>
	</header>

	<div id="wrap" class="container" role="document">
	<div id="content" class="row">
			<div id="main" class="span8" role="main">
				<div class="page-header">
					<h1>Zhiyang (Frank) Dou</h1>
<!--						<img class="size-medium wp-image-712" style="margin-bottom: 5px;" src="./files/my_name.png" alt="Homepage" width="100" />-->
<!--					</h1>-->
				</div>
				<div class="bio_container">
				<div class="slider-container">
					<div class="slider">


					  <div class="slide">
							<img class=" wp-image-697 alignleft dropshadow" style="margin: 0 1.5em 1em 0; border: 1px solid #222;" src="./files/Frank_avatar_style3.png"
							   alt="Slide 1" width="195" height="200">
					  </div>

						<div class="slide">
						  <img class="wp-image-697 alignleft dropshadow" style="margin: 0 -1.5em -1em 0; border: 1px solid #222;" src="./files/Frank_avatar_.png"
							   alt="Slide 2" width="195" height="200">

					  </div>

					</div>
					<div class="slider-controls">
					<button onclick="prevSlide()">&lt</button>
					<button onclick="nextSlide()">&gt</button>
					</div>


				</div>

					<div class="slider-text">
<!--									<div id="main" class="span8" role="main">-->
			<p>
					I am a Ph.D. candidate in Computer Graphics Group at <a href="https://www.hku.hk/">The University of Hong Kong</a>, supervised by <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Prof. Wenping Wang</a> and <a href="https://www.cs.hku.hk/index.php/people/academic-staff/taku">Prof. Taku Komura</a>. I received my B. Eng. degree with honors at <a href="https://www.en.sdu.edu.cn/">Shandong University</a>, advised by <a href="http://irc.cs.sdu.edu.cn/~shiqing/index.html">Prof. Shiqing Xin</a>. <br>
					I am currently a visiting scholar in the Department of Computer and Information Science at the <a href="https://www.upenn.edu/">University of Pennsylvania</a> working with <a href="https://lingjie0206.github.io/">Prof. Lingjie Liu</a> at Graphics Lab and <a href="https://www.grasp.upenn.edu/">GRASP Lab</a>.<br>
					My primary research is in Computer Science while intersecting with Environmental Science. I am an organizer of <a href="https://anysyn3d.github.io">AnySyn3D</a> research group, with a mission to enhance the speed, affordability, and quality of 3D AIGC üåé. I am also with Efficient Computing Interest Group.
<!--				I am also a co-founder and member of Efficient Computing Group (ECG).-->
<!--					<br>-->
				<p style="color: rgb(205,97,85)">üåüExpected graduation in 2025, open to postdoc and research scientist positions.üåü</p>
					Research Interests (<a href="#hiddenSection" onclick="scrollToHiddenSection()">Visualization</a>): Character Animation, Geometric Modeling and Processing, Simulation, Computer Graphics, Human Behavior Analysis (Capture, Modeling and Simulation).
<!--					<br>-->
				</p>

						<div class="alignbottom visible-desktop">
							<p>
								<a href="https://www.en.sdu.edu.cn/">
									<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/sdu.png" alt="Shandong University" width="55" />
								</a>
								&nbsp; &nbsp;
								<a href="http://www.hku.hk/">
									<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/hku1.png" alt="The University of Hong Kong" width="195" />
								</a>
								<a href="https://www.upenn.edu/">
<!--									# 							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/Logo_Upenn.png" alt="Upenn" height="48" />-->
									<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/Logo_Upenn.png" alt="University of Pennsylvania" width="141" />
								</a>

							</p>
						</div>

					</div>
				</div>







  <script>
    var slideIndex = 0;
    var slider = document.querySelector('.slider');
    var slides = document.querySelectorAll('.slide');

    function showSlide() {
      slider.style.transform = `translateX(-${slideIndex * 100}%)`;
    }

    function prevSlide() {
      slideIndex--;
      if (slideIndex < 0) {
        slideIndex = slides.length - 1;
      }
      showSlide();
    }

    function nextSlide() {
      slideIndex++;
      if (slideIndex >= slides.length) {
        slideIndex = 0;
      }
      showSlide();
    }
  </script>



				<!---
				</p>
				<p>
					</p><h3><i>Research Interests</i></h3>
					<table border="0" cellpadding="0" style="border:0pt;margin-left:8%">
						<tbody>
							<tr>
								<td style="border:0pt">
									<p align="center" style="text-align:center">
										<b>&nbsp;&nbsp;&nbsp;&nbsp;Computer Vision</b>
									</p>
								</td>
								<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td style="border:0pt">
									<p align="center" style="text-align:center">
										<b>&nbsp;&nbsp;&nbsp;&nbsp;Computer Graphics</b>
									</p>
								</td>
							</tr>
							<tr>
								<td style="border:0pt">
									<ul>
										<li style="text-align:left">Human Digitization</li>
										<li style="text-align:left">Image-based Reconstruction</li>
										<li style="text-align:left">Deep Learning</li>
									</ul>
								</td>
								<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td style="border:0pt">
									<ul>
										<li style="text-align:left">Texture/Pattern Synthesis</li>
										<li style="text-align:left">Geometric Modeling</li>
										<li style="text-align:left">Computational Fabrication</li>
									</ul>
								</td>
							</tr>
						</tbody>
					</table>
				<p></p>
				-->



				<!--- News -->
				<hr style="clear: both;">
				<h2>News</h2>
				<dl>
				<ul class="custom-list">
<!--					<li>-->
<!--						<span class="li_data">Jul. 2024:</span><span class="li_content">Coverage Axis is recognized at Top Cited Article in CGF 2022-2023.</span>-->
<!--					</li>-->
					<li>
						<span class="li_data">Jul. 2024:</span><span class="li_content">One co-first-authored journal paper accepted to SIGGRAPH Asia 2024.</span>
					  </li>
					  <li>
						<span class="li_data">Jul. 2024:</span><span class="li_content">Five papers accepted to ECCV 2024.</span>
					  </li>
					  <li>
						<span class="li_data">May. 2024:</span><span class="li_content">One paper accepted to SGP 2024.</span>
					  </li>
					  <li>
						<span class="li_data">Mar. 2024:</span><span class="li_content">One paper accepted to SIGGRAPH 2024.</span>
					  </li>
					  <li>
						<span class="li_data">Feb. 2024:</span><span class="li_content">One paper accepted to CVPR 2024.</span>
					  </li>
					  <li>
						<span class="li_data">Aug. 2023:</span><span class="li_content">One paper accepted to SIGGRAPH Asia 2023.</span>
					  </li>
					  <li>
						<span class="li_data">Jul. &ensp;2023:</span><span class="li_content">One paper accepted to ICCV 2023.</span>
					  </li>
					  <li>
						<span class="li_data">Mar. 2023:</span><span class="li_content">One paper accepted to SIGGRAPH 2023. We won <a href="https://blog.siggraph.org/2023/07/siggraph-2023-technical-papers-awards-best-papers-honorable-mentions-and-test-of-time.html/">SIGGRAPH 2023 The Best Paper Award</a>.</span>
					  </li>
					  <li>
						<span class="li_data">Mar. 2023:</span><span class="li_content">One paper accepted to PNAS Nexus 2023. <a href="https://www.eurekalert.org/news-releases/989780">Press release</a> by <a href="https://archive.eurekalert.org/aboutus.php">EurekAlert!</a>.</span>
					  </li>
					  <li>
						<span class="li_data">Aug. 2022:</span><span class="li_content">One paper accepted to SIGGRAPH Asia 2022.</span>
					  </li>
					  <li>
						<span class="li_data">Feb. 2022:</span><span class="li_content">One paper accepted to EUROGRAPHICS 2022.</span>
					  </li>
					</ul>
				</dl>


				<!--- Selected Publications -->
				<hr style="clear: both;">
				<!-- <h2>Selected Publications (<a href="https://frank-zy-d.github.io/publication.html">Full List</a>)</h2> -->
				<h2>Selected Publications</h2>
<!--				 * Equal contribution.-->
					     <p align="left">* Equal Contributions; # Corresponding Authors; cs: coming soon.</p>

<!--				<div class="row" style="margin-bottom: 1em;">-->
<!--&lt;!&ndash;					<div class="span3">&ndash;&gt;-->
<!--					     <p align="left">* Equal contribution; # Corresponding Author; cs: coming soon.</p>-->
<!--&lt;!&ndash;					</div>&ndash;&gt;-->
<!--				</div>-->
				<dl>

<!--					<div class="row">-->
<!--						<div class="span2" style="margin-bottom: 0em;">-->
<!--							<a href="https://frank-zy-dou.github.io/publication.html">-->
<!--								<img class="dropshadow pull-right" alt="" src="./figures/23_CAT++.png" width="200" height="120">-->
<!--							</a>-->
<!--						</div>-->
<!--						<div class="span6">-->
<!--							<a href="">-->
<!--								<strong>Coverage Axis++: Efficient Skeletal Points Selection for 3D Shape Skeletionization</strong>-->
<!--							</a>-->
<!--							<br> Zimeng Wang*, <strong>Zhiyang Dou</strong>*, Rui Xu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Shiqing Xin, Lingjie Liu, Taku Komura, Wenping Wang.<br>-->
<!--							<em> <b>Arxiv 2023.</b></em>-->
<!--							<br>-->
<!--							Prev. Work: <a href="https://frank-zy-dou.github.io/projects/CoverageAxis/index.html"><strong>Coverage Axis</strong></a>.-->
<!--							<br>-->
<!--							<ul class="inline middot">-->
<!--								<li><em class="fa fa-file"></em> <a href="">project page (cs)</a></li>-->
<!--								<li><em class="fa fa-file"></em> <a href="">paper (cs)</a></li>-->
<!--								<li><em class="fa fa-file"></em> <a href="">code (cs)</a></li>-->
<!--								<li>-->
<!--									<em class="fa fa-bars"></em>-->
<!--									<a class="toggle_text_link" onclick="javascript:toggle(cat++);">abstract</a>-->
<!--									<pre id="cat++" class="invisible_text"></pre> &lt;!&ndash; put </pre> on this line to avoid new unwanted line breaks after bibtex &ndash;&gt;-->
<!--								</li>-->
<!--							</ul>-->

<!--						</div>-->
<!--&lt;!&ndash;						<br>&ndash;&gt;-->
<!--					</div>-->

					<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/fig_24_dice.png" width="200" height="120">
							</a>
						</div>
						<div class="span6">
							<a href="">
								<strong>DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image</strong>
							</a>
							<br>Qingxuan Wu, <strong>Zhiyang Dou</strong>#, Sirui Xu, Soshi Shimada, Chen Wang, Zhengming Yu, Yuan Liu, Cheng Lin, Zeyu Cao, Taku Komura, Vladislav Golyanik, Christian Theobalt, Wenping Wang, Lingjie Liu#.
							<br>
							<em> <b>Arxiv 2024.</b></em>
							<br>
<!--							<br>-->
							<ul class="inline middot">
								<li><em class="fa fa-file"></em> <a href="https://frank-zy-dou.github.io/projects/DICE/index.html">project page</a></li>
								<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2406.17988">paper</a></li>
								<li><em class="fa fa-file"></em> <a href="https://github.com/Qingxuan-Wu/DICE">code</a></li>
								<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(dice);">abstract</a>
									<pre id="dice" class="invisible_text">Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face, complex deformations, and the ambiguity of the single-view setting. The first and only method for hand-face interaction recovery, Decaf, introduces a global fitting optimization guided by contact and deformation estimation networks trained on studio-collected data with 3D annotations. However, Decaf suffers from a time-consuming optimization process and limited generalization capability due to its reliance on 3D annotations of hand-face interaction data. To address these issues, we present DICE, the first end-to-end method for Deformation-aware hand-face Interaction reCovEry from a single image. DICE estimates the poses of hands and faces, contacts, and deformations simultaneously using a Transformer-based architecture. It features disentangling the regression of local deformation fields and global mesh vertex locations into two network branches, enhancing deformation and contact estimation for precise and robust hand-face mesh recovery. To improve generalizability, we propose a weakly-supervised training approach that augments the training set using in-the-wild images without 3D ground-truth annotations, employing the depths of 2D keypoints estimated by off-the-shelf models and adversarial priors of poses for supervision. Our experiments demonstrate that DICE achieves state-of-the-art performance on a standard benchmark and in-the-wild data in terms of accuracy and physical plausibility. Additionally, our method operates at an interactive rate (20 fps) on an Nvidia 4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our code will be publicly available upon publication.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								</li>
							</ul>

						</div>
					</div>

					<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/23_SurfD.png" width="200" height="120">
							</a>
						</div>
						<div class="span6">
							<a href="">
								<strong>Surf-D: High-Quality Surface Generation for Arbitrary Topologies using Diffusion Models</strong>
							</a>
							<br>Zhengming Yu*, <strong>Zhiyang Dou</strong>*, Xiaoxiao Long, Cheng Lin, Zekun Li, Yuan Liu, Norman M√ºller, Taku Komura, Marc Habermann, Christian Theobalt, Xin Li, Wenping Wang.
							<br>
							<em> <b>ECCV 2024.</b></em>
							<br>
<!--							<br>-->
							<ul class="inline middot">
								<li><em class="fa fa-file"></em> <a href="https://yzmblog.github.io/projects/SurfD/">project page</a></li>
								<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2311.17050">paper</a></li>
								<li><em class="fa fa-file"></em> <a href="https://github.com/Yzmblog/SurfD">code</a></li>
								<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(SurfD);">abstract</a>
									<pre id="SurfD" class="invisible_text">In this paper, we present Surf-D, a novel method for generating high-quality 3D shapes as Surface with arbitrary topologies using Diffusion models. Specifically, we adopt Unsigned Distance Field (UDF) as the surface representation, as it excels in handling arbitrary topologies, enabling the generation of complex shapes. While the prior methods explored shape generation with different representations, they suffer from limited topologies and geometry details. Moreover, it's non-trivial to directly extend prior diffusion models to UDF because they lack spatial continuity due to the discrete volume structure. However, UDF requires accurate gradients for mesh extraction and learning. To tackle the issues, we first leverage a point-based auto-encoder to learn a compact latent space, which supports gradient querying for any input point through differentiation to effectively capture intricate geometry at a high resolution. Since the learning difficulty for various shapes can differ, a curriculum learning strategy is employed to efficiently embed various surfaces, enhancing the whole embedding process. With pretrained shape latent space, we employ a latent diffusion model to acquire the distribution of various shapes. Our approach demonstrates superior performance in shape generation across multiple modalities and conducts extensive experiments in unconditional generation, category conditional generation, 3D reconstruction from images, and text-to-shape tasks. Our code will be publicly available upon paper publication.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								</li>
							</ul>

						</div>
					</div>
<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/23_EMDM.png" width="200" height="120">
							</a>
						</div>
						<div class="span6">
							<a href="">
								<strong>EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Human Motion Generation</strong>
							</a>
							<br>Wenyang Zhou, <strong>Zhiyang Dou</strong>‚Ä†, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, Lingjie Liu.<br>
							<em> <b>ECCV 2024.</b></em>
							<br>
							‚Ä† Project Lead.
							<br>
							<ul class="inline middot">
								<li><em class="fa fa-file"></em> <a href="https://frank-zy-dou.github.io/projects/EMDM/index.html">project page</a></li>
								<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2312.02256">paper</a></li>
								<li><em class="fa fa-file"></em> <a href="https://youtu.be/1SyCXbnol_g?si=ExYgmuM1eXB0-Yjj">video</a></li>
								<li><em class="fa fa-file"></em> <a href="https://github.com/Frank-ZY-Dou/EMDM">code</a></li>
								<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(emdm);">abstract</a>
									<pre id="emdm" class="invisible_text">We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Although previous motion diffusion models have shown impressive results, they struggle to achieve fast generation while maintaining high-quality human motions. Motion latent diffusion has been proposed for efficient motion generation. However, effectively learning a latent space can be non-trivial in such a two-stage manner. Meanwhile, accelerating motion sampling by increasing the step size, e.g., DDIM, typically leads to a decline in motion quality due to the inapproximation of complex data distributions when naively increasing the step size. In this paper, we propose EMDM that allows for much fewer sample steps for fast motion generation by modeling the complex denoising distribution during multiple sampling steps. Specifically, we develop a Conditional Denoising Diffusion GAN to capture multimodal data distributions conditioned on both control signals, i.e., textual description and denoising time step. By modeling the complex data distribution, a larger sampling step size and fewer steps are achieved during motion synthesis, significantly accelerating the generation process. To effectively capture the human dynamics and reduce undesired artifacts, we employ motion geometric loss during network training, which improves the motion quality and training efficiency. As a result, EMDM achieves a remarkable speed-up at the generation stage while maintaining high-quality motion generation in terms of fidelity and diversity.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								</li>
							</ul>

						</div>
					</div>

					<div style="height: 0.6em;"><br></div>

					<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/23_TLControl.png" width="200" height="120">
							</a>
						</div>
						<div class="span6">
							<a href="">
								<strong>TLControl: Trajectory and Language Control for Human Motion Synthesis</strong>
							</a>
							<br>Weilin Wan, <strong>Zhiyang Dou</strong>, Taku Komura, Wenping Wang, Dinesh Jayaraman, Lingjie Liu.<br>
							<em> <b>ECCV 2024.</b></em>
							<br>

							<br>
							<ul class="inline middot">
								<li><em class="fa fa-file"></em> <a href="https://tlcontrol.weilinwl.com/">project page</a></li>
								<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2311.17135">paper</a></li>
								<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=thN1sQwP8R4&ab_channel=William">video</a></li>
								<li><em class="fa fa-file"></em> <a href="https://github.com/HiWilliamWWL/TLControl">code</a></li>
								<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(tlcontrol);">abstract</a>
									<pre id="tlcontrol" class="invisible_text">Controllable human motion synthesis is essential for applications in AR/VR, gaming, movies, and embodied AI. Existing methods often focus solely on either language or full trajectory control, lacking precision in synthesizing motions aligned with user-specified trajectories, especially for multi-joint control. To address these issues, we present TLControl, a new method for realistic human motion synthesis, incorporating both low-level trajectory and high-level language semantics controls. Specifically, we first train a VQ-VAE to learn a compact latent motion space organized by body parts. We then propose a Masked Trajectories Transformer to make coarse initial predictions of full trajectories of joints based on the learned latent motion space, with user-specified partial trajectories and text descriptions as conditioning. Finally, we introduce an efficient test-time optimization to refine these coarse predictions for accurate trajectory control. Experiments demonstrate that TLControl outperforms the state-of-the-art in trajectory accuracy and time efficiency, making it practical for interactive and high-quality animation generation.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								</li>
							</ul>

						</div>
<!--						<br>-->
					</div>

					<br>




					<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/23_Dist_Human.png" width="200" height="120">
							</a>
						</div>
						<div class="span6">
							<a href="">
								<strong>Disentangled Clothed Avatar Generation from Text Descriptions</strong>
							</a>
							<br>Jionghao Wang, Yuan Liu, <strong>Zhiyang Dou</strong>, Zhengming Yu, Yongqing Liang, Xin Li, Wenping Wang, Rong Xie, Li Song.<br>
							<em> <b>ECCV 2024.</b></em>
							<br>
							<br>
							<ul class="inline middot">
									<li><em class="fa fa-file"></em> <a href="https://shanemankiw.github.io/SO-SMPL/">project page</a></li>
								<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2312.05295">paper</a></li>
								<li><em class="fa fa-file"></em> <a href="https://github.com/shanemankiw/SO-SMPL">code</a></li>
									<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(dist_human);">abstract</a>
									<pre id="dist_human" class="invisible_text">In this paper, we introduced a novel text-to-avatar generation method that separately generates the human body and the clothes and allows high-quality animation on the generated avatar. While recent advancements in text-to-avatar generation have yielded diverse human avatars from text prompts, these methods typically combine all elements‚Äîclothes, hair, and body‚Äîinto a single 3D representation. Such an entangled approach poses challenges for downstream tasks like editing or animation. To overcome these limitations, we propose a novel disentangled 3D avatar representation named Sequentially Offset-SMPL (SO-SMPL), building upon the SMPL model. SO-SMPL represents the human body and clothes with two separate meshes, but associates them with offsets to ensure the physical alignment between the body and the clothes. Then, we design an Score Distillation Sampling(SDS)-based distillation framework to generate the proposed SO-SMPL representation from text prompts. In comparison with existing text-to-avatar methods, our approach not only achieves higher exture and geometry quality and better semantic alignment with text prompts, but also significantly improves the visual quality of character animation, virtual try-on, and avatar editing.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								</li>
							</ul>
						</div>
					</div>



					<br>
<!--				<div class="row">-->
<!--						<div class="span2" style="margin-bottom: 0em;">-->
<!--							<a href="https://frank-zy-dou.github.io/publication.html">-->
<!--								<img class="dropshadow pull-right" alt="" src="./figures/fig_3Dtrack.png" width="200" height="120">-->
<!--							</a>-->
<!--						</div>-->
<!--						<div class="span6">-->
<!--							<a href="">-->
<!--								<strong>Boosting 3D Single Object Tracking with 2D Matching Distillation and 3D Pre-training</strong>-->
<!--							</a>-->
<!--							<br>Qiangqiang Wu, Yan Xia, <strong>Zhiyang Dou</strong>, Jia Wan, Antoni Chan.<br>-->
<!--							<em> <b>ECCV 2024.</b></em>-->
<!--							<br>-->
<!--							<br>-->
<!--							<ul class="inline middot">-->
<!--								<li><em class="fa fa-file"></em> <a href="">paper (cs)</a></li>-->
<!--								<li><em class="fa fa-file"></em> <a href="">code (cs)</a></li>-->
<!--									<li>-->
<!--									<em class="fa fa-bars"></em>-->
<!--									<a class="toggle_text_link" onclick="javascript:toggle(track3d);">abstract</a>-->
<!--									<pre id="track3d" class="invisible_text">3D single object tracking (SOT) is an essential task in autonomous driving and robotics. However, learning robust 3D SOT trackers remains challenging due to the limited category-specific point cloud data and the inherent sparsity and incompleteness of LiDAR scans. To tackle these issues, we propose a unified 3D SOT framework that leverages 3D generative pre-training and learns robust 3D matching abilities from 2D pre-trained foundation trackers. Our framework features a consistent target-matching architecture with the widely used 2D trackers, facilitating the transfer of 2D matching knowledge. Specifically, we first propose a lightweight Target-Aware Projection (TAP) module, allowing the pre-trained 2D tracker to work well on the projected point clouds without further fine-tuning. We then propose a novel IoU-guided matching-distillation framework that utilizes the powerful 2D pre-trained trackers to guide 3D matching learning in the 3D tracker, i.e., the 3D template-to-search matching should be consistent with its corresponding 2D template-to-search matching obtained from 2D pre-trained trackers. Our designs are applied to two mainstream 3D SOT frameworks: memory-less Siamese and contextual memory-based approaches, which are respectively named SiamDisst and MemDisst. Extensive experiments show that SiamDisst and MemDisst achieve state-of-the-art performance on KITTI, Waymo Open Dataset and nuScenes benchmarks, while running at the above real-time speed of 25 and 90 FPS on a single RTX3090 GPU. The code will be made publicly available.</pre> &lt;!&ndash; put </pre> on this line to avoid new unwanted line breaks after bibtex &ndash;&gt;-->
<!--								</li>-->
<!--							</ul>-->
<!--						</div>-->
<!--					</div>-->
<!--			<br>-->


					<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/23_CAT++.png" width="200" height="120">
							</a>
						</div>
						<div class="span6">
							<a href="">
								<strong>Coverage Axis++: Efficient Skeletal Points Selection for 3D Shape Skeletonization</strong>
							</a>
							<br> Zimeng Wang*, <strong>Zhiyang Dou</strong>*, Rui Xu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Shiqing Xin, Taku Komura, Xiaoming Yuan, Wenping Wang.<br>
							<em> <b> Eurographics/ACM SIGGRAPH Symposium on Geometry Processing 2024.</b></em>
							<br>
							A follow-up of  <a href="https://frank-zy-dou.github.io/projects/CoverageAxis/index.html">Coverage Axis</a>.

							<br>
							<ul class="inline middot">
								<li><em class="fa fa-file"></em> <a href="https://frank-zy-dou.github.io/projects/CoverageAxis++/index.html">project page</a></li>
								<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2401.12946">paper</a></li>
								<li><em class="fa fa-file"></em> <a href="https://github.com/Frank-ZY-Dou/Coverage_Axis">code</a></li>
								<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(cat2);">abstract</a>
									<pre id="cat2" class="invisible_text">
We introduce Coverage Axis++, a novel and efficient approach to 3D shape skeletonization. The current state-of-the-art approaches for this task often rely on the watertightness of the input or suffer from substantial computational costs, thereby limiting their practicality. To address this challenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal points, offering a high-accuracy approximation of the Medial Axis Transform (MAT) while significantly mitigating computational intensity for various shape representations. We introduce a simple yet effective strategy that considers both shape coverage and uniformity to derive skeletal points. The selection procedure enforces consistency with the shape structure while favoring the dominant medial balls, which thus introduces a compact underlying shape representation in terms of MAT. As a result, Coverage Axis++ allows for skeletonization for various shape representations (e.g., water-tight meshes, triangle soups, point clouds), specification of the number of skeletal points, few hyperparameters, and highly efficient computation with improved reconstruction accuracy. Extensive experiments across a wide range of 3D shapes validate the efficiency and effectiveness of Coverage Axis++. The code will be publicly available once the paper is published.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								</li>
							</ul>

						</div>
<!--						<br>-->
					</div>
<!--					<br>-->
					<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/fig_sig24_part123.jpg" width="200" height="120">
							</a>
						</div>
						<div class="span6">
							<a href="">
								<strong>Part123: Part-aware 3D Reconstruction from a Single-view Image</strong>
							</a>
							<br>Anran Liu*, Cheng Lin* , Yuan Liu, Xiaoxiao Long, <strong>Zhiyang Dou</strong>, Haoxiang Guo, Ping Luo, Wenping Wang.
							<br>
							<em> <b>SIGGRAPH 2024.</b></em>
							<br>

							<br>
							<ul class="inline middot">
								<li><em class="fa fa-file"></em> <a href="https://liuar0512.github.io/part123_official_page/">project page</a></li>
								<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2405.16888">paper</a></li>
<!--								<li><em class="fa fa-file"></em> <a href="https://youtu.be/Cgq6JbQ1VW4">video</a></li>-->
								<li><em class="fa fa-file"></em> <a href="">code (cs)</a></li>
								<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(part123);">abstract</a>
									<pre id="part123" class="invisible_text">Recently, the emergence of diffusion models has opened up new opportunities for single-view reconstruction. However, all the existing methods represent the target object as a closed mesh devoid of any structural information, thus neglecting the part-based structure, which is crucial for many downstream applications, of the reconstructed shape. Moreover, the generated meshes usually suffer from large noises, unsmooth surfaces, and blurry textures, making it challenging to obtain satisfactory part segments using 3D segmentation techniques. In this paper, we present Part123, a novel framework for part-aware 3D reconstruction from a single-view image. We first use diffusion models to generate multiview-consistent images from a given image, and then leverage Segment Anything Model (SAM), which demonstrates powerful generalization ability on arbitrary objects, to generate multiview segmentation masks. To effectively incorporate 2D part-based information into 3D reconstruction and handle inconsistency, we introduce contrastive learning into a neural rendering framework to learn a part-aware feature space based on the multiview segmentation masks. A clustering-based algorithm is also developed to automatically derive 3D part segmentation results from the reconstructed models. Experiments show that our method can generate 3D models with high-quality segmented parts on various objects. Compared to existing unstructured reconstruction methods, the part-aware 3D models from our method benefit some important applications, including feature-preserving reconstruction, primitive fitting, and 3D shape editing.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								</li>
							</ul>

						</div>
<!--						<br>-->
					</div>
					<br>
					<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/23_wonder3D_.png" width="200" height="120">
							</a>
						</div>
						<div class="span6">
							<a href="">
								<strong>Wonder3D: Single Image to 3D using Cross-Domain Diffusion</strong>
							</a>
							<br>Xiaoxiao Long*, Yuanchen Guo*, Cheng Lin, Yuan Liu, <strong>Zhiyang Dou</strong>, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, Wenping Wang.<br>
							<em> <b>CVPR 2024.</b></em>
							<br>

							<br>
							<ul class="inline middot">
								<li><em class="fa fa-file"></em> <a href="https://www.xxlong.site/Wonder3D/">project page</a></li>
								<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2310.15008">paper</a></li>
<!--								<li><em class="fa fa-file"></em> <a href="https://youtu.be/Cgq6JbQ1VW4">video</a></li>-->
								<li><em class="fa fa-file"></em> <a href="https://github.com/xxlong0/Wonder3D">code</a></li>
								 <li><em class="fa fa-file"></em> <a href="https://huggingface.co/spaces/flamehaze1115/Wonder3D-demo">Hugging Face Demo</a></li>
								<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
								<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(wonder3d);">abstract</a>
									<pre id="wonder3d" class="invisible_text">In this work, we introduce Wonder3D, a novel method for efficiently generating high-fidelity textured meshes from single-view images.Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of image-to-3D tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure consistency, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a geometry-aware normal fusion algorithm that extracts high-quality surfaces from the multi-view 2D representations. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and reasonably good efficiency compared to prior works.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								</li>
							</ul>

						</div>
<!--						<br>-->
					</div>

					<br>




<!--					<div style="height: 0.6em;"><br></div>-->


					<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/23_SIGA_EASE.png" width="200" height="120">
							</a>
						</div>
						<div class="span6">
							<a href="">
								<strong>C¬∑ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters</strong>
							</a>
							<br><strong>Zhiyang Dou</strong>, Xuelin Chen, Qingnan Fan, Taku Komura, Wenping Wang.<br>
							<em> <b>SIGGRAPH Asia 2023.</b></em>
							<br>

							<br>
							<ul class="inline middot">
								<li><em class="fa fa-file"></em> <a href="https://frank-zy-dou.github.io/projects/CASE/index.html">project page</a></li>
								<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2309.11351">paper</a></li>
								<li><em class="fa fa-file"></em> <a href="https://youtu.be/Cgq6JbQ1VW4">video</a></li>
								<li><em class="fa fa-file"></em> <a href="https://github.com/Frank-ZY-Dou/CASE">code</a></li>
								<!-- <li><em class="fa fa-file"></em> <a href="https://diglib.eg.org/bitstream/handle/10.1111/cgf14484/paper1026_1.pdf?sequence=2&isAllowed=y">suppl.</a></li> -->
								<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
								<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(ease);">abstract</a>
									<pre id="ease" class="invisible_text">We present C¬∑ASE, an efficient and effective framework that learns Conditional Adversarial Skill Embeddings for physics-based characters. C¬∑ASE enables the physically simulated character to learn a diverse repertoire of skills while providing controllability in the form of direct manipulation of the skills to be performed. This is achieved by dividing the heterogeneous skill motions into distinct subsets containing homogeneous samples for training a low-level conditional model to learn the conditional behavior distribution. The skill-conditioned imitation learning naturally offers explicit control over the character‚Äôs skills after training. The training course incorporates the focal skill sampling, skeletal residual forces, and element-wise feature masking to balance diverse skills of varying complexities, mitigate dynamics mismatch to master agile motions and capture more general behavior characteristics, respectively. Once trained, the conditional model can produce highly diverse and realistic skills, outperforming state-of-the-art models, and can be repurposed in various downstream tasks. In particular, the explicit skill control handle allows a high-level policy or a user to direct the character with desired skill specifications, which we demonstrate is advantageous for interactive character animation.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								</li>
							</ul>

						</div>
<!--						<br>-->
					</div>

					<br>

					<div class="row">
					<div class="span2" style="margin-bottom: 0em;">
						<a href="https://frank-zy-dou.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/23_ICCV_TORE.png" width="200" height="120">
						</a>
					</div>
					<div class="span6">
						<a href="">
							<strong>TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer</strong>
						</a>
						<br><strong>Zhiyang Dou</strong>*, Qingxuan Wu*, Cheng Lin, Zeyu Cao, Qiangqiang Wu, Weilin Wan, Taku Komura, Wenping Wang.<br>
						<em> <b> ICCV 2023.</b></em>
						<br>
						&nbsp;
						<!-- <em>"a learnable implicit representation for modeling both closed and open surfaces"</em> -->
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://frank-zy-dou.github.io/projects/Tore/index.html">project page</a></li>
							<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2211.10705">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://github.com/Frank-ZY-Dou/TORE">code</a></li>
							<!-- <li><em class="fa fa-file"></em> <a href="https://diglib.eg.org/bitstream/handle/10.1111/cgf14484/paper1026_1.pdf?sequence=2&isAllowed=y">suppl.</a></li> -->
							<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(tore);">abstract</a>
								<pre id="tore" class="invisible_text">In this paper, we introduce a set of simple yet effective TOken REduction (TORE) strategies for Transformer-based Human Mesh Recovery from monocular images. Current SOTA performance is achieved by Transformer-based structures. However, they suffer from high model complexity and computation cost caused by redundant tokens. We propose token reduction strategies based on two important aspects, i.e., the 3D geometry structure and 2D image feature, where we hierarchically recover the mesh geometry with priors from body structure and conduct token clustering to pass fewer but more discriminative image feature tokens to the Transformer. Our method massively reduces the number of tokens involved in high-complexity interactions in the Transformer. This leads to a significantly reduced computational cost while still achieving competitive or even higher accuracy in shape recovery. Extensive experiments across a wide range of benchmarks validate the superior effectiveness of the proposed method. We further demonstrate the generalizability of our method on hand mesh recovery. Our code will be publicly available once the paper is published.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>


					</div>
<!--						<br>-->
				</div>
					<br>


					<!--start from here-->
					<div class="row">
					<div class="span2" style="margin-bottom: 0em;">
						<a href="https://frank-zy-dou.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/23_SIG_GCNO.png" width="200" height="120">
						</a>
					</div>
					<div class="span6" style="margin-bottom: 0em;">
						<a href="">
							<strong>Globally Consistent Normal Orientation for Point Clouds by Regularizing the Winding-Number Field</strong>
						</a>
						<br>Rui Xu, <strong>Zhiyang Dou</strong>, Ningna Wang,  Shiqing Xin, Shuangmin Chen, Mingyan Jiang, Xiaohu Guo, Wenping Wang, Changhe Tu.<br>
						<em> <b>ACM Transactions on Graphics. SIGGRAPH 2023.</b></em>
						<p style="color: rgb(205,97,85)"><strong>SIGGRAPH 2023 Best Paper Award; See more <a href="https://blog.siggraph.org/2023/07/siggraph-2023-technical-papers-awards-best-papers-honorable-mentions-and-test-of-time.html/">here</a>.</strong></p>

						<!-- <em>"a learnable implicit representation for modeling both closed and open surfaces"</em> -->
<!--						<br>-->
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://xrvitd.github.io/Projects/GCNO/index.html">project page</a></li>
							<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2304.11605">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=08pOt5JqWJE&ab_channel=N">video</a></li>
							<li><em class="fa fa-file"></em> <a href="https://github.com/Xrvitd/GCNO">code</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(GCNO);">abstract</a>
								<pre id="GCNO" class="invisible_text">Estimating normals with globally consistent orientations for a raw point cloud has many downstream geometry processing applications. Despite tremendous efforts in the past decades, it remains challenging to deal with an unoriented point cloud with various imperfections, particularly in the presence of data sparsity coupled with nearby gaps or thin-walled structures. In this paper, we propose a smooth objective function to characterize the requirements of an acceptable winding-number field, which allows one to find the globally consistent normal orientations starting from a set of completely random normals. By taking the vertices of the Voronoi diagram of the point cloud as examination points, we consider the following three requirements: (1) the winding number is either 0 or 1, (2) the occurrences of 1 and the occurrences of 0 are balanced around the point cloud, and (3) the normals align with the outside Voronoi poles as much as possible. Extensive experimental results show that our method outperforms the existing approaches, especially in handling sparse and noisy point clouds, as well as shapes with complex geometry/topology.</pre>
							</li>

						</ul>
					</div>
				</div>
<!--					<br>-->
				<div class="row">
					<div class="span2" style="margin-bottom: 0em;">
						<a href="https://frank-zy-dou.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/22_SIGA_rfeps.png" width="200" height="120">
						</a>
					</div>
					<div class="span6" style="margin-bottom: 0em;">
						<a href="">
							<strong>RFEPS: Reconstructing Feature-line Equipped Polygonal Surface</strong>
						</a>
						<br>Rui Xu, Zixiong Wang, <strong>Zhiyang Dou</strong>, Chen Zong, Shiqing Xin, Mingyan Jiang, Tao Ju, Changhe Tu.<br>
						<em> <b>ACM Transactions on Graphics. SIGGRAPH Asia 2022.</b></em>
						<br>
						<!-- <em>"a learnable implicit representation for modeling both closed and open surfaces"</em> -->
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://xrvitd.github.io/Projects/RFEPS/index.html">project page</a></li>
							<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2212.03600">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=iRP5z-JOCEc&ab_channel=%E5%BE%90%E7%91%9E">video</a></li>
							<li><em class="fa fa-file"></em> <a href="https://github.com/Xrvitd/RFEPS">code</a></li>
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(rfeps);">abstract</a>
								<pre id="rfeps" class="invisible_text">Feature lines are important geometric cues in characterizing the structure of a CAD model. Despite great progress in both explicit reconstruction and implicit reconstruction, it remains a challenging task to reconstruct a polygonal surface equipped with feature lines, especially when the input point cloud is noisy and lacks faithful normal vectors. In this paper, we develop a multistage algorithm, named RFEPS, to address this challenge. The key steps include (1)denoising the point cloud based on the assumption of local planarity, (2)identifying the feature-line zone by optimization of discrete optimal transport, (3)augmenting the point set so that sufficiently many additional points are generated on potential geometry edges, and (4) generating a polygonal surface that interpolates the augmented point set based on restricted power diagram. We demonstrate through extensive experiments that RFEPS, benefiting from the edge-point augmentation and the feature-preserving explicit reconstruction, outperforms state-of-the-art methods in terms of the reconstruction quality, especially in terms of the ability to reconstruct missing feature lines.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>

						</ul>
					</div>
				</div>
					<br>
<!--					<div id="hiddenSection3" class="hidden-anchor"></div>-->
					<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/22_EG_CAT.png" width="200" height="120">
							</a>
						</div>
						<div class="span6">
<!--							<a href=""  class="textwidget" id="anotherText3" >-->
							<a href="" >
								<strong>Coverage Axis: Inner Point Selection for 3D Shape Skeletonization</strong>
							</a>
							<br><strong>Zhiyang Dou</strong>, Cheng Lin, Rui Xu, Lei Yang, Shiqing Xin, Taku Komura, Wenping Wang.<br>
							<em> <b>Computer Graphics Forum. EUROGRAPHICS 2022.</b></em>
							<p style="color: rgb(205,97,85)"><strong>Top Cited Article in CGF 2022-2023. [<a href="https://frank-zy-dou.github.io/files/Cert_Top_Cited_Article.pdf">Link</a>]</strong>
								<br><strong>Fast-Forward Attendees Award at EG22, 2nd Place.</strong></p>
	<!--						<br>-->
							<!-- <em>"a learnable implicit representation for modeling both closed and open surfaces"</em> -->
	<!--						<br>-->
							<ul class="inline middot">
								<li><em class="fa fa-file"></em> <a href="https://frank-zy-dou.github.io/projects/CoverageAxis/index.html">project page</a></li>
								<li><em class="fa fa-file"></em> <a href="https://arxiv.org/abs/2110.00965">paper</a></li>
								<li><em class="fa fa-file"></em> <a href="https://github.com/Frank-ZY-Dou/Coverage_Axis">code</a></li>
								<li><em class="fa fa-file"></em> <a href="https://diglib.eg.org/bitstream/handle/10.1111/cgf14484/paper1026_1.pdf?sequence=2&isAllowed=y">suppl.</a></li>
								<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
								<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(cat);">abstract</a>
									<pre id="cat" class="invisible_text">In this paper, we present a simple yet effective formulation called Coverage Axis for 3D shape skeletonization. Inspired by the set cover problem, our key idea is to cover all the surface points using as few inside medial balls as possible. This formulation inherently induces a compact and expressive approximation of the Medial Axis Transform (MAT) of a given shape. Different from previous methods that rely on local approximation error, our method allows a global consideration of the overall shape structure, leading to an efficient high-level abstraction and superior robustness to noise. Another appealing aspect of our method is its capability to handle more generalized input such as point clouds and poor-quality meshes. Extensive comparisons and evaluations demonstrate the remarkable effectiveness of our method for generating compact and expressive skeletal representation to approximate the MAT.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
								</li>
							</ul>
						</div>
					</div>
					<br>
<div class="row">
							<div class="span2" style="margin-bottom: 0em;">
								<a href="https://frank-zy-dou.github.io/publication.html">
									<img class="dropshadow pull-right" alt="" src="./figures/23_STE_.png" width="200" height="120">
								</a>
							</div>
							<div class="span6">
								<a href="">
									<strong>Analysis of SARS-CoV-2 Transmission in a University Classroom based on Real Human Close Contact Behaviors</strong>
								</a>
								<br>Nan Zhang, Xueze Yang, Boni Su, <strong>Zhiyang Dou</strong>#.<br>
								<em> <b>Science of the Total Environment (STOTEN) 2024.</b></em>
								<br>
								# Corresponding Author.
								<br>
		<!--						<em>System development for close contact behavior collection.</em>-->
		<!--						<p style="color: rgb(205,97,85)"><strong>This research has been featured in a <a href="https://www.eurekalert.org/news-releases/989780">press release</a> by <a href="https://archive.eurekalert.org/aboutus.php" >EurekAlert!</a>.</strong></p>-->
		<!--							<br>
		-->
								<ul class="inline middot">
		<!--							<li><em class="fa fa-file"></em> <a href="https://frank-zy-dou.github.io/projects/Pnas_nexus23/index.html">project page</a></li>-->
									<li><em class="fa fa-file"></em> <a href="">paper</a></li>
		<!--							<li><em class="fa fa-file"></em> <a href="">press release</a></li>-->
									<!-- <li><em class="fa fa-file"></em> <a href="https://github.com/Xrvitd/RFEPS">code</a></li> -->
									<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
										<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(scs23_STE);">abstract</a>
									<pre id="scs23_STE" class="invisible_text">Under Review.</pre>
								</li>
								</ul>
							</div>
					</div>
					<br>

					<div class="row">
						<div class="span2" style="margin-bottom: 0em;">
							<a href="https://frank-zy-dou.github.io/publication.html">
								<img class="dropshadow pull-right" alt="" src="./figures/23_SCS_HST.png" width="200" height="120">
							</a>
						</div>
						<div class="span6">
							<a href="">
								<strong>Popularization of High-Speed Railway Reduces the Infection Risk via Close Contact Route during Journey</strong>
							</a>
							<br>Nan Zhang, Xiyue Liu, Shuyi Gao, Boni Su, <strong>Zhiyang Dou</strong>#.<br>
							<em> <b>Sustainable Cities and Society (SCS) 2023.</b></em>
							<br>
							# Corresponding Author.
							<br>
	<!--						<em>System development for close contact behavior collection.</em>-->
	<!--						<p style="color: rgb(205,97,85)"><strong>This research has been featured in a <a href="https://www.eurekalert.org/news-releases/989780">press release</a> by <a href="https://archive.eurekalert.org/aboutus.php" >EurekAlert!</a>.</strong></p>-->
	<!--							<br>

	-->
							<ul class="inline middot">
	<!--							<li><em class="fa fa-file"></em> <a href="https://frank-zy-dou.github.io/projects/Pnas_nexus23/index.html">project page</a></li>-->
								<li><em class="fa fa-file"></em> <a href="">paper</a></li>
	<!--							<li><em class="fa fa-file"></em> <a href="">press release</a></li>-->
								<!-- <li><em class="fa fa-file"></em> <a href="https://github.com/Xrvitd/RFEPS">code</a></li> -->
								<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
								<li>
									<em class="fa fa-bars"></em>
									<a class="toggle_text_link" onclick="javascript:toggle(scs23_HST);">abstract</a>
									<pre id="scs23_HST" class="invisible_text">The risk of COVID-19 infection has increased due to the prolonged duration of travel and frequent close interactions due to popularization of railway transportations. This study utilized depth detection devices to analyze the close contact behaviors of passengers in high-speed train (HST), traditional trains (TT), waiting area in waiting room (WWR), and ticket check area in waiting room (CWR). A multi-route COVID-19 transmission model was developed to assess the risk of virus exposure in these scenarios under various non-pharmaceutical interventions. A total of 163,740 seconds of data was collected. The close contact ratios in HST, TT, WWR, and CWR was 5.8%, 64.0%, 7.7%, and 49.0%, respectively. The average interpersonal distance between passengers was 0.85 m, 0.92 m, 1.25 m, and 0.88 m, respectively. The probability of face-to-face contact was 9.5%, 70.0%, 64.2%, and 5.8% across each environment, respectively. When all passengers wore N95 respirators and surgical masks, the personal virus exposure via close contact can be reduced by 94.1% and 51.9%, respectively. The virus exposure in TT is about dozens of times of it in HST. In China, if all current railway traffic was replaced by HST, the total virus exposure of passengers can be reduced by roughly 50%. </pre>
								</li>
							</ul>
						</div>
					</div>
					<br>
<!--					<div class="row">-->
<!--						<div class="span2" style="margin-bottom: 0em;">-->
<!--							<a href="https://frank-zy-dou.github.io/publication.html">-->
<!--								<img class="dropshadow pull-right" alt="" src="./figures/23_JOBE_airport.png" width="200" height="120">-->
<!--							</a>-->
<!--						</div>-->
<!--						<div class="span6">-->
<!--							<a href="">-->
<!--								<strong>Analysis of SARS-CoV-2 Transmission in Airports based on Real Human Close Contact Behaviors</strong>-->
<!--							</a>-->
<!--							<br>Xueze Yang*, <strong>Zhiyang Dou</strong>*, Yuqing Ding, Boni Su, Hua Qian, Nan Zhang.<br>-->
<!--							<em> <b>Journal of Building Engineering (JOBE) 2023</b>.</em>-->
<!--							<br>-->
<!--							<br>-->
<!--							<ul class="inline middot">-->
<!--								<li><em class="fa fa-file"></em> <a href="">paper</a></li>-->
<!--	&lt;!&ndash;							<li><em class="fa fa-file"></em> <a href="">press release</a></li>&ndash;&gt;-->
<!--								&lt;!&ndash; <li><em class="fa fa-file"></em> <a href="https://github.com/Xrvitd/RFEPS">code</a></li> &ndash;&gt;-->
<!--								&lt;!&ndash;<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> &ndash;&gt;-->
<!--								<li>-->
<!--									<em class="fa fa-bars"></em>-->
<!--									<a class="toggle_text_link" onclick="javascript:toggle(jobe23);">abstract</a>-->
<!--									<pre id="jobe23" class="invisible_text">The COVID-19 pandemic has significantly impacted people's daily lives for over three years. Airports, with their dense population and frequent close contact, pose a higher risk of respiratory infectious diseases compared to many other indoor environments. However, limited availability of data on close contact behavior has resulted in a gap in indoor exposure analysis. This study conducted depth sensor measurements and video data collection across nine areas of a northern (airport A) and a southern (airport B) airports in China by 11 participants. The data, comprising more than 44 hours of close contact behaviors, including interpersonal distance, relative facial orientation, and the relative position of individuals, were analyzed using a semi-supervised machine learning method. Based on this analysis, a close contact transmission model for COVID-19 was developed, which considers the aforementioned close contact behaviors to assess the risk of exposure and the efficacy of interventions. The average close contact ratio in 9 airport‚Äôs areas is 25.4% (ranging from 6.1% to 55.0%), with passengers having the highest frequency of close contact in manual check-in areas. During close contacts, the average interpersonal distance in airports is 1.2 meters (ranging from 1.1 to 1.4 meters), being shortest in boarding areas. Face-to-face close contact is highest in charging areas, with a percentage of 46.9%. If people maintain a distance of over 1.0 meter in all areas, the total virus exposure could be reduced by 6.9% to 22.0% compared to the actual situation. Dining areas have the highest virus exposure risk for both short-range inhalation and mucosal deposition, followed by manual check-in areas. This study provides a data support for the scientific epidemic prevention and control in airports from the viewpoint of close contact behaviors.</pre>-->
<!--								</li>-->
<!--							</ul>-->
<!--						</div>-->
<!--					</div>-->

<!--					<br>-->


					<div class="row">
					<div class="span2" style="margin-bottom: 0em;">
						<a href="https://frank-zy-dou.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/23_PNAS_Nexus_student.png" width="200" height="120">
						</a>
					</div>
					<div class="span6">
						<a href="">
							<strong>Student close contact behavior and COVID-19 transmission in China‚Äôs classrooms</strong>
						</a>

						<br>Yong Guo*, <strong>Zhiyang Dou</strong>*, Nan Zhang, Xiyue Liu, Boni Su, Yuguo Li, Yinping Zhang.<br>
						<em> <b>PNAS Nexus 2023.</b></em>
<!--						<br>-->

<!--						<em>System development for close contact behavior collection.</em>-->
						<p style="color: rgb(205,97,85)"><strong>This research has been featured in a <a href="https://www.eurekalert.org/news-releases/989780">press release</a> by <a href="https://archive.eurekalert.org/aboutus.php" >EurekAlert!</a>.</strong></p>
<!--							<br>

-->
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://frank-zy-dou.github.io/projects/Pnas_nexus23/index.html">project page</a></li>
							<li><em class="fa fa-file"></em> <a href="https://academic.oup.com/pnasnexus/article/2/5/pgad142/7175271">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://www.eurekalert.org/news-releases/989780">press release</a></li>
							<!-- <li><em class="fa fa-file"></em> <a href="https://github.com/Xrvitd/RFEPS">code</a></li> -->
							<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(pnasnexus23);">abstract</a>
								<pre id="pnasnexus23" class="invisible_text">Classrooms are high-risk indoor environments, so analysis of SARS-CoV-2 transmission in classrooms is important for determining optimal interventions. Due to the absence of human behavior data, it is challenging to accurately determine virus exposure in classrooms. A wearable device for close contact behavior detection was developed, and we recorded more than 250-thousand data points of close contact behaviors of students from Grades 1 through 12. Combined with a survey on students‚Äô behaviors, we analyzed virus transmission in classrooms. Close contact rates for students were 37%¬±11% during classes and 48%¬±13% during breaks. Students in lower grades had higher close contact rates and virus transmission potential. The long-range airborne transmission route is dominant, accounting for 90%¬±3.6% and 75%¬±7.7% with and without mask wearing, respectively. During breaks, the short-range airborne route became more important, contributing 48%¬±3.1% in grades 1 to 9 (without wearing masks). Ventilation alone cannot always meet the demands of COVID-19 control, 30 m3/h/person is suggested as the threshold outdoor air ventilation rate in classroom. This study provides scientific support for COVID-19 prevention and control in classrooms, and our proposed human behavior detection and analysis methods offer a powerful tool to understand virus transmission characteristics, and can be employed in various indoor environments.</pre>
							</li>
						</ul>
					</div>
				</div>
					<br>



				<div class="row">
					<div class="span2" style="margin-bottom: 0em;">
						<a href="https://frank-zy-dou.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/22_EST_close.png" width="200" height="120">
						</a>
					</div>
					<div class="span6">
						<a href="">
							<strong>Close Contact Behaviors of University and School Students in 10 Typical Indoor Environments</strong>
						</a>

						<br> Nan Zhang, Li Liu, <strong>Zhiyang Dou</strong>, Xiyue Liu, Xueze Yang, Doudou Miao, Yong Guo, Silan Gu, Yuguo Li, Hua Qian, Jianjian Wei.<br>
						<em> <b>Journal of Hazardous Materials (JHM) 2023.</b></em>
						<br>
<!--			one paper ac			<em>System development for close contact behavior collection.</em>-->
<!--						<br>-->
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://www.sciencedirect.com/science/article/pii/S0304389423013523?dgcid=coauthor">paper</a></li>
							<li><em class="fa fa-file"></em> <a href="https://www.sciencedirect.com/science/article/pii/S0304389423013523?dgcid=coauthor">paper</a></li>
							<!-- <li><em class="fa fa-file"></em> <a href="https://github.com/Xrvitd/RFEPS">code</a></li> -->
							<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(est);">abstract</a>
								<pre id="est" class="invisible_text">Close contact, including both short-range airborne and large droplet, is recognized as the main route of SARS-CoV-2 transmission in indoor environments, however exposure risk via this route is difficult to quantify due to a lack of data showing close contact behaviors of people in typical indoor environments. A digital wearable device was developed to capture human close contact behaviors automatically based on semi-supervised learning. We collected a total of 337,056 seconds of indoor close contacts from 194 and a half hours of depth video recordings in 10 typical indoor environments. The relationship between SARS-CoV-2 exposure and close contact behaviors were evaluated based on dispersion characteristics of virus-laden droplets. People in restaurant had the highest close contact ratio (63.8%) and probability of face-to-face pattern (77.6%) during close contacts, while people in shopping center had the highest speak fraction (46.6%). University students had higher exposure potential in dormitories than school students in homes, but less exposure potential in classrooms and graduate student offices than school students in classrooms. Aerosol exposure in volume for both short-range inhalation and direct deposition on facial mucosa were highest in restaurants. Classroom is the main indoor environment for SARS-CoV-2 transmission for school students. The obtained results based on real human close contact behaviors can be used for infection risk assessment and to deploy effective interventions against close contact transmission of COVID-19 and other respiratory infections.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
					<br>

				<div class="row">
					<div class="span2" style="margin-bottom: 0em;">
						<a href="https://frank-zy-dou.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/22_JHM_metro.png" width="200" height="120">
						</a>
					</div>
					<div class="span6">
						<a href="">
							<strong>Close Contact Behavior-based COVID-19 Transmission and Interventions in a Subway System</strong>
						</a>
						<br>Xiyue Liu*, <strong>Zhiyang Dou</strong>*, Lei Wang, Boni Su, Tianyi Jin, Yong Guo, Jianjian Wei, Nan Zhang.<br>
						<em> <b>Journal of Hazardous Materials (JHM) 2022.</b></em>
						<br>
<!--						<em>Vision system development for human behavior collection and analysis.</em>-->
<!--						<br>-->
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://frank-zy-dou.github.io/projects/Metro_JHM22/index.html">project page</a></li>
							<li><em class="fa fa-file"></em> <a href="https://www.sciencedirect.com/science/article/pii/S0304389422010238">paper</a></li>
							<!-- <li><em class="fa fa-file"></em> <a href="">code (coming soon)</a></li> -->
							<!-- <li><em class="fa fa-file"></em> <a href="https://diglib.eg.org/bitstream/handle/10.1111/cgf14484/paper1026_1.pdf?sequence=2&isAllowed=y">suppl.</a></li> -->
							<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(jhm_metro);">abstract</a>
								<pre id="jhm_metro" class="invisible_text">During COVID-19 pandemic, analysis on virus exposure and intervention efficiency in public transports based on real passenger‚Äôs close contact behaviors is critical to curb infectious disease transmission. A monitoring device was developed to gather a total of 145,821 close contact data in subways based on semi-supervision learning. A virus transmission model considering both short- and long-range inhalation and deposition was established to calculate the virus exposure. During rush-hour, short-range inhalation exposure is 3.2 times higher than deposition exposure and 7.5 times higher than long-range inhalation exposure of all passengers in the subway. The close contact rate was 56.1 % and the average interpersonal distance was 0.8 m. Face-to-back was the main pattern during close contact. Comparing with random distribution, if all passengers stand facing in the same direction, personal virus exposure through inhalation (deposition) can be reduced by 74.1 % (98.5 %). If the talk rate was decreased from 20 % to 5 %, the inhalation (deposition) exposure can be reduced by 69.3 % (73.8 %). In addition, we found that virus exposure could be reduced by 82.0 % if all passengers wear surgical masks. This study provides scientific support for COVID-19 prevention and control in subways based on real human close contact behaviors.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
					<br>



				<div class="row">
					<div class="span2" style="margin-bottom: 0em;">
						<a href="https://frank-zy-dou.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/20_TVCG_shape_abs.png" width="200" height="120">
						</a>
					</div>
					<div class="span6">
						<a href="">
							<strong>Top-Down Shape Abstraction Based on Greedy Pole Selection</strong>
						</a>
						<br><strong>Zhiyang Dou</strong>, Shiqing Xin, Rui Xu, Jian Xu, Yuanfeng Zhou, Shuangmin Chen, Wenping Wang, Xiuyang Zhao, Changhe Tu.<br>
						<em> <b>IEEE Transactions on Visualization and Computer Graphics. TVCG 2020.</b></em>
						<br>
						<!-- <em>In this paper, I developed vision system for human behavior collection and analysis.</em> -->
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://ieeexplore.ieee.org/document/9095378">paper</a></li>
							<!-- <li><em class="fa fa-file"></em> <a href="">code (coming soon)</a></li> -->
							<!-- <li><em class="fa fa-file"></em> <a href="https://diglib.eg.org/bitstream/handle/10.1111/cgf14484/paper1026_1.pdf?sequence=2&isAllowed=y">suppl.</a></li> -->
							<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(tvcg_topdown);">abstract</a>
								<pre id="tvcg_topdown" class="invisible_text">Motivated by the fact that the medial axis transform is able to encode nearly the complete shape, we propose to use as few medial balls as possible to approximate the original enclosed volume by the boundary surface. We progressively select new medial balls, in a top-down style, to enlarge the region spanned by the existing medial balls. The key spirit of the selection strategy is to encourage large medial balls while imposing given geometric constraints. We further propose a speedup technique based on a provable observation that the intersection of medial balls implies the adjacency of power cells (in the sense of the power crust). We further elaborate the selection rules in combination with two closely related applications. One application is to develop an easy-to-use ball-stick modeling system that helps non-professional users to quickly build a shape with only balls and wires, but any penetration between two medial balls must be suppressed. The other application is to generate porous structures with convex, compact (with a high isoperimetric quotient) and shape-aware pores where two adjacent spherical pores may have penetration as long as the mechanical rigidity can be well preserved.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
					<br>




				</dl>


				<!--- News -->
				<hr style="clear: both;">
				<h2>Research Experience and Arrangements</h2>
				<dl>
				<ul>
					<li>
						<p>
							<strong>Nov. 2023 - Oct. 2024</strong>: Visiting Scholar,
							<strong><a href="https://www.upenn.edu/">University of Pennsylvania (UPenn)</a></strong>.
						</p>
					</li>
					<li>
						<p>
							<strong>Jul.&ensp;2023 - Nov. 2023</strong>: Research Intern,
							<strong><a href="https://twitter.com/TencentGames"> Tencent Games</a></strong>.
						</p>
					</li>

					<li>
						<p>
							<strong>Jun. 2023 - Jul. 2023</strong>: Visiting Scholar,
							<strong><a href="https://english.bjut.edu.cn/"> Beijing University of Technology (BJUT)</a></strong>.
						</p>
					</li>
					<li>
						<p>
							<strong>Apr. 2022 - Jun. 2023</strong>: Research Intern,
							<strong><a href="https://ai.tencent.com/ailab/en/about/"> Tencent AI Lab</a></strong>.
						</p>
					</li>

					<li>
						<p>
							<strong>Jul. 2019 - Oct. 2019</strong>: Research Assistant,
							<strong> <a href="https://www.cs.hku.hk/"> University of Hong Kong (HKU)</a></strong>.
						</p>
					</li>


					<li>
						<p>
							<strong>Mar. 2018 - Jun. 2019</strong>: Research Assistant (part-time),
							<strong><a href="http://irc.cs.sdu.edu.cn/">  Interdisciplinary Research Center (IRC)</a></strong>.
						</p>
					</li>


				</ul>
				</dl>
				<h3>Work Experience and Collaboration</h3>
				<br>
				<div class="alignbottom">
					<p>
						<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/fig_collaboration.png" alt="" width="94%" />

<!--						<a href="http://irc.cs.sdu.edu.cn/">-->
<!--							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/irc.png" alt="IRC" height="53" />-->
<!--						</a>-->
<!--						<a href="https://www.cs.hku.hk/">-->
<!--							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/hku1.png" alt="The University of Hong Kong" height=53" />-->
<!--						</a>-->
<!--						<tr>-->
<!--						<a href="https://ai.tencent.com/ailab/en/about/">-->
<!--							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/tencent_ailab.jpg" alt="Tencent AI Lab" height="48" />-->
<!--						</a>-->
<!--							&nbsp;-->
<!--							&nbsp;-->
<!--							&nbsp;-->
<!--							<a href="https://english.bjut.edu.cn/">-->
<!--							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/BJUT.png" alt="BJUT" height="53" />-->
<!--						</a>-->

<!--							&nbsp;-->
<!--							&nbsp;-->

<!--							<a href="https://twitter.com/TencentGames">-->
<!--							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/Logo_tencentgame.png" alt="Tencentgame" height="53" />-->
<!--						</a>-->
<!--							<br>-->
<!--							<br>-->
<!--							&nbsp;-->
<!--							<a href="https://www.transgp.hk/">-->
<!--							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/TransGP.png" alt="Boeing" height="40" />-->
<!--						</a>-->
<!--							&nbsp;-->
<!--							&nbsp;-->

<!--								<a href="http://www.boeing.cn/">-->
<!--							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/boeing.png" alt="Boeing" height="48" />-->
<!--						</a>-->
<!--							&nbsp;-->
<!--							&nbsp;-->
<!--							<a href="https://www.upenn.edu/">-->
<!--							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/Logo_Upenn.png" alt="Upenn" height="47" />-->
<!--						</a>-->






				<!-- 		<a href="https://www.inria.fr/en/">
							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/inria.png" alt="Inria" width="110" />
						</a>

						<a href="http://ict.usc.edu/">
							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/uscict.jpg" alt="USC ICT" width="130" />
						</a>
						<a href="https://www.tencent.com/en-us/index.html">
							<img class="size-medium wp-image-712" style="margin-bottom: 0px;" src="./files/tencent.png" alt="TecentUS" width="90" />
						</a>
				 -->
					</p>
				</div>




				<hr style="clear: both;">

				<h2>Services</h2>
				<br>
<!--				<dl>-->
<!--				<ul>-->
<!--					<li>-->
						<p>
							<strong> Reviewer</strong>: SIGGRAPH; SIGGRAPH ASIA; TVCG; ICCV; CVPR; ECCV; 3DV; CVPRW; ECCVW; PG; EG (P); GM; CAD (CADJ); TIP; GMP; CGI; Graphics Replicability Stamp; ICONIP; FSDM; MLIS; Scientific (BrainSTEM@HKU).
						</p>
<!--					</li>-->

<!--					<li>-->
<!--						<p>-->
							<strong> Teaching Assistant:</strong>
<!--						</p>-->

<!--							-->
											<dl>
					<ul class="custom-list">
									<li>
										<span class="li_data_short">2023:</span><span class="li_content_short">COMP3271 Computer Graphics. Worked with <a href="https://www.cs.hku.hk/index.php/people/academic-staff/taku"> Prof. Taku Komura</a>.</span>
									</li>
									<li>
										<span class="li_data_short">2022:</span><span class="li_content_short">COMP3362 Hands-on AI: Experimentation and Applications. Worked with <a href="https://www.cs.hku.hk/people/academic-staff/ykchoi">Dr. Yi-King Choi</a>.</span>
									</li>
									<li>
										<span class="li_data_short">2021:</span><span class="li_content_short">COMP3362 Hands-on AI: Experimentation and Applications. Worked with <a href="https://www.cs.hku.hk/people/academic-staff/ykchoi">Dr. Yi-King Choi</a>.</span>
									</li>
									<li>
										<span class="li_data_short">2020:</span><span class="li_content_short">COMP2120 Computer Organization. Worked with <a href="https://www.cs.hku.hk/people/academic-staff/kpchan">Prof. Kwok-Ping Chan</a>.</span>
									</li>

								</ul>
								</dl>

<!--					</li>-->

<!--					<li>-->
						<p>
							<strong> Invited Talks:</strong>
						</p>

						<dl>
					<ul class="custom-list">
									<li>
										<span class="li_data">Apr. 2024:</span><span class="li_content">On the Readily Deployable System for Detecting Close Contact Behaviors, <a href="http://www.boeing.cn/">Boeing</a>.</span>
									</li>
									<li>
										<span class="li_data">Nov. 2023:</span><span class="li_content">Geometric Computing - Medial Axis Transform and Normal Orientation for Point Clouds, <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a>.</span>
									</li>
									<li>
										<span class="li_data">Oct. 2023:</span><span class="li_content">Scalable Skill Embeddings for Physics-based Characters, <a href="https://twitter.com/TencentGames">Tencent Games</a>.</span>
									</li>
									<li>
										<span class="li_data">Jun. 2023:</span><span class="li_content">Robust and Efficient Vision Systems for Close Contact Behavior Analysis, <a href="https://english.bjut.edu.cn/">Beijing University of Technology</a>.</span>
									</li>
									<li>
										<span class="li_data">Feb. 2023:</span><span class="li_content">Scalable Skill Embeddings for Physics-based Characters, <a href="https://www.en.sdu.edu.cn/">Shandong University</a>.</span>
									</li>
									<li>
										<span class="li_data">Oct. 2022:</span><span class="li_content">On Efficient Hand-to-Surface Contact Estimation, <a href="http://www.boeing.cn/">Boeing</a>.</span>
									</li>
								</ul>
						</dl>

<!--					</li>-->
<!--				</ul>-->
<!--				</dl>-->
			<hr style="clear: both;">
				<h2>Awards, Scholarships and Honors</h2>
				<dl>
					<ul class="custom-list">

							<li>
							<span class="li_data">Jul. 2024:</span><span class="li_content">Top Cited Article in CGF 2022-2023. [<a href="https://frank-zy-dou.github.io/files/Cert_Top_Cited_Article.pdf">Link</a>]</span>
							</li>
							<li>
							<span class="li_data">Jul. 2024:</span><span class="li_content">HKU Foundation First Year Excellent PhD Award 2023/24.</span>
							</li>
							<li>
								<span class="li_data">Oct. 2023:</span><span class="li_content">The Best Paper Award, SIGGRAPH 2023. [<a href="https://frank-zy-dou.github.io/files/Cert_SIG23_BP.pdf">Link</a>]</span>
							</li>
							<li>
								<span class="li_data">Oct. 2020:</span><span class="li_content">Postgraduate Scholarship.</span>
							</li>
							<li>
								<span class="li_data">Oct. 2019:</span><span class="li_content">National Scholarship.</span>
							</li>
							<li>
								<span class="li_data">Dec. 2019:</span><span class="li_content">Presidential Scholarship.</span>
							</li>
							<li>
								<span class="li_data">Oct. 2018:</span><span class="li_content">National Scholarship.</span>
							</li>


					</ul>
				</dl>
				<h2>Competitions</h2>
				<dl>
					<ul class="custom-list">
						<li>
							<span class="li_data_short">2019:</span><span class="li_content_short">National First Prize, National Mathematical Modeling Contest.</span>
						</li>
						<li>
							<span class="li_data_short">2019:</span><span class="li_content_short">Meritorious Winner, International Mathematical Modeling Contest: The Mathematical Contest in Modeling (MCM).</span>
						</li>
						<li>
							<span class="li_data_short">2018:</span><span class="li_content_short">National First Prize, The Best Paper Award (8/38573), National Mathematical Modeling Contest.</span>
						</li>
						<li>
							<span class="li_data_short">2018:</span><span class="li_content_short">Meritorious Winner, International Mathematical Modeling Contest: The Interdisciplinary Contest in Modeling (ICM).</span>
						</li>
						<li>
							<span class="li_data_short">2018:</span><span class="li_content_short">National Grand Prize (1st), Most Commercially Valuable Award, Most Popular Award; The 11th National University Student Software Innovation Contest.</span>
						</li>

					</ul>
				</dl>



			</div><!-- /#main -->
			<aside id="sidebar" class="span4" role="complementary">
				<div class="well">
					<section id="text-2" class="widget-1 widget-first widget widget_text">
						  <div id="hiddenSection" class="hidden-anchor"></div>
						  <div id="hiddenSection2" class="hidden-anchor"></div>
							<script>
								function scrollToHiddenSection() {
								  var element = document.getElementById('hiddenSection');

								  // ‰ΩøÁî® block: 'start' Âíå inline: 'nearest' Á°Æ‰øùÁõÆÊ†áÂÖÉÁ¥†Á®çÂæÆÂÅèÁ¶ªÈ°∂ÈÉ®
								  element.scrollIntoView({ behavior: 'smooth', block: 'start', inline: 'nearest' });

								  var anotherText = document.getElementById('anotherText');

									  // ÂàáÊç¢Âä†Á≤óÊïàÊûú
									  element.classList.toggle('bold');

									  // ‰ΩøÂè¶‰∏Ä‰∏™Âú∞ÊñπÁöÑÊñáÂ≠óÈó™ÁÉÅ
									  anotherText.classList.add('highlighted');

									  // 3ÁßíÂêéÁßªÈô§Èó™ÁÉÅÊïàÊûú
									  setTimeout(function () {
										anotherText.classList.remove('highlighted');
									  }, 1000);
								}
							</script>

							<script>
								function scrollToHiddenSection2() {
								  var element = document.getElementById('hiddenSection2');
								  element.scrollIntoView({ behavior: 'smooth', block: 'start', inline: 'nearest' });
								  var anotherText = document.getElementById('anotherText2');

									  // ÂàáÊç¢Âä†Á≤óÊïàÊûú
									  element.classList.toggle('bold');

									  // ‰ΩøÂè¶‰∏Ä‰∏™Âú∞ÊñπÁöÑÊñáÂ≠óÈó™ÁÉÅ
									  anotherText.classList.add('highlighted');

									  // 3ÁßíÂêéÁßªÈô§Èó™ÁÉÅÊïàÊûú
									  setTimeout(function () {
										anotherText.classList.remove('highlighted');
									  }, 1000);
								}
							  </script>

							  <script>
								function scrollToHiddenSection3() {
								  var element = document.getElementById('hiddenSection3');
								  element.scrollIntoView({ behavior: 'smooth', block: 'start', inline: 'nearest' });
								  var anotherText = document.getElementById('anotherText3');
									  // ÂàáÊç¢Âä†Á≤óÊïàÊûú
									  element.classList.toggle('bold');
									  // ‰ΩøÂè¶‰∏Ä‰∏™Âú∞ÊñπÁöÑÊñáÂ≠óÈó™ÁÉÅ
									  anotherText.classList.add('highlighted');
									  // 3ÁßíÂêéÁßªÈô§Èó™ÁÉÅÊïàÊûú
									  setTimeout(function () {
										anotherText.classList.remove('highlighted');
									  }, 1000);
								}
							  </script>


						<div class="widget-inner">
							<h3>Visualization of Research Interests</h3>
							<br>
							<center>
<!--  							<img src="files/fig_roadmap.png" style="width: 292px;">-->
  							<img src="files/fig_roadmap2.png" style="width: 292px;">

							</center>
							<center>
								<p style="margin-top: 8px;">Research Interest Overview.</p>
																		<br>
							<i><p style="font-size: small;", style="font-family: 'Times New Roman', serif;">
								"... And now I see with eye serene<br>
The very pulse of the machine;<br>
A Being breathing thoughtful breath,<br>
A Traveller between life and death;"<br>
								...<br>
								"The sacred geometry of chance;<br>
						The hidden law of a probable outcome;<br>
								The numbers lead a dance;..."<br>
							</p></strong>
							</i>
																		<br>

				</center>
							<h3>Contact info</h3>
							<div class="textwidget"><h6 id="anotherText2">Email</h6>
								<a class="email" href="mailto:zhiyang0@connect.hku.hk" >zhiyang0@connect.hku.hk</a>
								<br>
								<a class="email" href="mailto:zydou@seas.upenn.edu">zydou@seas.upenn.edu</a>
								<br>
								<a class="email" href="mailto:frankzydou@gmail.com">frankzydou@gmail.com</a>
								<br>
								<br>
								<h6>Address</h6>
<!--								&#x2022; 34F 032, Tencent AILab, Gemdale Viseen Tower B, Hi-Tech South 9th Rd, Shenzhen, China.-->
<!--								<br>-->
<!--								&#x2022; Rm 416, CYC Bldg, The University of Hong Kong, Pokfulam Road, Hong Kong.-->
								&#x2022; Rm 475, Levine Hall, 3330 Walnut Street, Philadelphia, PA 19104, U.S.A.
								<br>
								&#x2022; Rm 416, CYC Bldg, The University of Hong Kong, Pokfulam Road, Hong Kong SAR, China.



							</div>
						</div>
					</section>

					<br>
					<section id="text-2" class="widget-1 widget-first widget widget_text">
						<div class="widget-inner">
							<h3> <a href="./files/CV_Zhiyang_Dou.pdf"> Curriculum Vitae </a></h3>
						</div>
					</section>
					<br>

					<section id="social-widget-2" class="widget-3 widget Social_Widget">
						<div class="widget-inner">
							<h3>More Links</h3>
							<div class="socialmedia-buttons smw_left">
								<a href="https://scholar.google.com/citations?user=SLRYlKsAAAAJ&hl=en" rel="publisher" target="_blank">
									<img width="27" height="27" src="./files/scholar.png" alt="Google Scholar" title="Google Scholar" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
								</a>
								<a href="https://github.com/Frank-ZY-Dou" rel="nofollow" target="_blank">
									<img width="27" height="27" src="./files/github.png" alt="GitHub" title="GitHub" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
								</a>
								<a href="https://www.linkedin.com/in/zhiyang-dou-0259111b3/" rel="nofollow" target="_blank">
									<img width="27" height="27" src="./files/linkedin.png" alt="LinkedIn" title="LinkedIn" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
								</a>

								<a href="https://www.researchgate.net/profile/Zhiyang-Dou-2" rel="nofollow" target="_blank">
									<img width="27" height="27" src="./files/RG.jpg" alt="RG" title="RG" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
								</a>


								<a href="https://orcid.org/0000-0003-0186-8269" rel="nofollow" target="_blank">
									<img width="28" height="28" src="./files/orcid.png" alt="ORCID" title="ORCID" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
								</a>
								<a href="https://www.semanticscholar.org/author/Zhiyang-Dou/1382485620" rel="publisher" target="_blank">
									<img width="27" height="27" src="./files/semanticscholar.png" alt="Semantic Scholar" title="Semantic Scholar" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
								</a>

									<a href="https://twitter.com/frankzydou" rel="nofollow" target="_blank">
									<img width="27" height="27" src="./files/twitter.png" alt="Twitter" title="Twitter" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
									</a>


									<a href="https://www.youtube.com/@ZhiyangDou" rel="nofollow" target="_blank">
									<img width="28" height="27" src="./files/Youtube_logo.png" alt="Youtube" title="Youtube" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
									</a>
									<ul class="nav">
<!--								&#x2022; 34F 032, Tencent AILab, Gemdale Viseen Tower B, Hi-Tech South 9th Rd, Shenzhen, China.-->
<!--								<br>-->
<!--								&#x2022; Rm 416, CYC Bldg, The University of Hong Kong, Pokfulam Road, Hong Kong.-->
								<br>
								&#x2022; <a href="http://anysyn3d.github.io">AnySyn3D</a>
								<br>
								&#x2022; <a href="https://hku-cg.github.io/">CGVU Lab</a>








							<!-- 	<a href="https://www.facebook.com/chen.weikai.3" rel="nofollow" target="_blank">
									<img width="32" height="32" src="./files/facebook.png" alt="Follow me on Facebook" title="Follow me on Facebook" style="opacity: 0.8; -moz-opacity: 0.8;" class="fade">
								</a> -->

								<!-- site credits -->
								<br><br>
								<em class="fa fa-credit-card"></em> <a class="toggle_text_link" onclick="javascript:toggle(site_credits);">Site Credits</a>
								<pre id="site_credits" class="invisible_text">
This site was built using <a href="http://twitter.github.io/bootstrap/">Bootstrap</a>, a front-end framework for web development. <br /><br />The style of this site is inspired by <a href="http://richardt.name/">Dr. Christian Richardt</a>'s, <a href="http://herohuyongtao.github.io/">Dr. Yongtao Hu</a>'s and <a href="http://chenweikai.github.io/">Dr. Weikai Chen</a>'s personal websites.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->

								<!-- site statistics -->
								<!-- <br><br> -->
								<!-- <script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=563zgztxfth&amp;s=186&amp;m=6&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script> -->
							</div>
						</div>
					</section>
				</div>
			</aside><!-- /#sidebar -->
		</div><!-- /#content -->
	</div><!-- /#wrap -->

	<footer id="content-info" class="container" role="contentinfo">
		<table width="100%">
			<tbody><tr>
				<td style="text-align:center">
					<div style="display:inline-block;width:210px;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/7.js?i=563zgztxfth&amp;m=6&amp;c=007eff&amp;cr1=ffffff&amp;br=8&amp;sx=0" async="async"></script>
				</div>
				<p class="copy"><small>Last updated: May, 2024</small></p>
					<p class="copy"><small>¬© 2024 Zhiyang Dou</small></p>
			 </td>
			</tr>


		</tbody></table>
	</footer>

	<!-- From http://stackoverflow.com/a/11668413/72470
	<script>
	  !function ($) {
		$(function(){
		  window.prettyPrint && prettyPrint()
		})
	  }(window.jQuery)
	</script>
	-->


<div id="cntvlive2-is-installed"></div></body></html>