<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<title>MotionWavelet: Human Motion Prediction via Wavelet Manifold Learning</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<!-- Meta tags for Zotero grab citation -->
	<meta name="citation_title" content="MotionWavelet: Human Motion Prediction via Wavelet Manifold Learning">
	<meta name="citation_publication_date" content="2024">
	<meta name="citation_conference_title" content="Arxiv 2024">
	<meta name="citation_pdf_url" content="">

	<meta name="robots" content="index,follow">
	<meta name="description"
		content="">
	<link rel="author" href="" />


	<!-- Fonts and stuff -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800'
		rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
	<link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
	<script src="js/google-code-prettify/prettify.js"></script>



	<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
});
</script>



</head>


<body>
	<div id="content">
		<div id="content-inner">
<!--			<div class="section logos" style="text-align:center">-->
<!--				<a href="https://www.cam.ac.uk/" target="_blank"><IMG src="./logos/Cambridge.png" height="66" border="0"></a>-->
<!--				</td>-->
<!--				<a href="https://www.hku.hk/" target="_blank"><IMG src="./logos/Logo_HKU.png" height="68" border="0"></a>-->
<!--				</td>-->
<!--				<a href="https://www.shlab.org.cn/" target="_blank"><IMG src="./logos/SAIL.png" height="62" border="0"></a>-->
<!--				</td>-->
<!--				<a href="https://www.tamu.edu/" target="_blank"><IMG src="./logos/Logo_TAMU.png" height="62" border="4"></a></td>-->

<!--				<a href="https://www.upenn.edu/" target="_blank"><IMG src="./logos/penn.png" height="66"-->
<!--						border="4"></a></td>-->
<!--			</div>-->

			<div class="section head">
<!--				<br>-->
<!--				<br>-->
				<br>

				<h1> <img src="figs/MotionWavelet.svg" alt="pic" style="height: 36px; vertical-align: top;"> MotionWavelet: Human Motion Prediction via <br>Wavelet Manifold Learning</h1>

				<br>
				<div class="authors">

					<a href="">Yuming Feng</a><sup> 1,†</sup>&#160;&#160;
					<a href="https://frank-zy-dou.github.io/" target="_blank">Zhiyang Dou</a><sup> 2,3,†,‡</sup>&#160;&#160;
					<a href="https://lhchen.top/" target="_blank">Ling-Hao Chen</a><sup> 4</sup>&#160;&#160;
					<a href="https://liuyuan-pal.github.io/" target="_blank">Yuan Liu</a><sup> 5,6</sup>&#160;&#160;
					<a href="https://easypapersniper.github.io/" target="_blank">Tianyu Li</a><sup> 7</sup>&#160;&#160;
					<a href="https://scholar.google.co.uk/citations?user=GStTsxAAAAAJ&hl=zh-CN" target="_blank">Jingbo Wang</a><sup> 8</sup>&#160;&#160;
					<a href="https://scholar.google.com/citations?user=MiIz8tYAAAAJ&hl=en" target="_blank">Zeyu Cao</a><sup> 9</sup>&#160;&#160;<br>
					<a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank">Wenping Wang</a><sup> 10</sup>&#160;&#160;
					<a href="https://i.cs.hku.hk/~taku/" target="_blank">Taku Komura</a><sup> 2</sup>&#160;&#160;
					<a href="https://lingjie0206.github.io/" target="_blank">Lingjie Liu</a><sup> 3,‡</sup>&#160;&#160;
				</div>

<!--				<br>-->
				<div class="affiliations">
						<sup>1</sup><a href="https://www.imperial.ac.uk/" target="_blank">ICL</a>&#160;&#160;
					<sup>2</sup><a href="https://www.hku.hk/" target="_blank">HKU</a>&#160;&#160;
					<sup>3</sup><a href="https://www.upenn.edu/" target="_blank">UPenn</a>&#160;&#160;
					<sup>4</sup><a href="https://www.tsinghua.edu.cn/en/" target="_blank">THU</a>&#160;&#160;
					<sup>5</sup><a href="https://hkust.edu.hk/" target="_blank">HKUST</a>&#160;&#160;
					<sup>6</sup><a href="https://www.ntu.edu.sg/" target="_blank">NTU</a>&#160;&#160;<br>
					<sup>7</sup><a href="https://www.gatech.edu/" target="_blank">Georgia Tech</a>&#160;&#160;
					<sup>8</sup><a href="https://www.shlab.org.cn/" target="_blank">Shanghai AI Lab</a>&#160;&#160;
					<sup>9</sup><a href="https://www.cam.ac.uk/" target="_blank">Cambridge</a>&#160;&#160;
					<sup>10</sup><a href="https://www.tamu.edu/" target="_blank">TAMU</a>&#160;&#160;
<!--					<sup>1</sup><a href="https://www.imperial.ac.uk/" target="_blank">Imperial College London</a>&#160;&#160;-->
<!--					<sup>2</sup><a href="https://www.hku.hk/" target="_blank">The University of Hong Kong</a>&#160;&#160;-->
<!--					<sup>3</sup><a href="https://www.upenn.edu/" target="_blank">University of Pennsylvania</a>&#160;&#160;-->
<!--					<sup>4</sup><a href="https://www.tsinghua.edu.cn/en/" target="_blank">Tsinghua University</a>&#160;&#160;<br>-->
<!--					<sup>5</sup><a href="https://hkust.edu.hk/" target="_blank">Hong Kong University of Science and Technology</a>&#160;&#160;-->
<!--					<sup>6</sup><a href="https://www.ntu.edu.sg/" target="_blank">Nanyang Technological University</a>&#160;&#160;<br>-->
<!--					<sup>7</sup><a href="https://www.gatech.edu/" target="_blank">Georgia Institute of Technology</a>&#160;&#160;-->
<!--					<sup>8</sup><a href="https://www.shlab.org.cn/" target="_blank">Shanghai AI Laboratory</a>&#160;&#160;-->
<!--					<sup>9</sup><a href="https://www.cam.ac.uk/" target="_blank">University of Cambridge</a>&#160;&#160;-->
<!--					<sup>10</sup><a href="https://www.tamu.edu/" target="_blank">Texas A&M University</a>&#160;&#160;-->
				</div>

								<div class="authors"> †, ‡ denote equal contributions and corresponding authors.</div>
				<div class="venue">Arxiv 2024. </div>
			</div>
						<div class="section downloads">
					<center>
						<ul style="padding-left: 0">
							<li class="grid">
								<div class="griditem">
									<a href="https://arxiv.org/abs/2411.16964"><img src="images/pdf.png"></a><br/>
									<a href="https://arxiv.org/abs/2411.16964">Paper</a>
								</div>
							</li><li class="grid">
								<div class="griditem">
									<a href="https://youtu.be/LaRH5ssjsr8?si=rxFsJgbiLhmCF9Fe"><img src="images/video.png"></a><br/>
									<a href="https://youtu.be/LaRH5ssjsr8?si=rxFsJgbiLhmCF9Fe">Video</a>
								</div>
							</li>
<!--							<li class="grid">-->
<!--								<div class="griditem">-->
<!--									<a href=""><img src="images/data_ico.png"></a><br/>-->
<!--									<a href="">Code</a>-->
<!--								</div>-->
<!--							</li>-->
						</ul>
					</center>
				</div>

<!--			<div class="section abstract">-->
<!--					<center>-->
<!--						<strong>Please check out our <a href="https://youtu.be/Cgq6JbQ1VW4">video</a> for more details.</strong>-->
<!--					</center>-->
<!--			</div>-->
			<div class="section downloads">
				<center>
						<iframe width="830" height="467" src="https://www.youtube.com/embed/pyWq0OYJdI0?si=KKr11fnbq0l4YO7K" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
				</center>
			</div>

			<div class="section abstract">
				<h2>Abstract</h2><br>
				<p>Modeling temporal characteristics and the non-stationary dynamics of body movement plays a significant role in predicting human future motions. However, it is challenging to capture these features due to the subtle transitions involved in the complex human motions. This paper introduces MotionWavelet, a human motion prediction framework that utilizes Wavelet Transformation and studies the human motion patterns in the spatial-frequency domain. In MotionWavelet, a Wavelet Diffusion Model (WDM) learns a Wavelet Manifold by applying Wavelet Transformation on the motion data therefore encoding the intricate spatial and temporal motion patterns. Once the Wavelet Manifold is built, WDM trains a diffusion model to generate human motions from Wavelet latent vectors. In addition to the WDM, MotionWavelet also presents a Wavelet Space Shaping Guidance mechanism to refine the denoising process to improve conformity with the manifold structure. WDM also develops Temporal Attention-Based Guidance to enhance the prediction accuracy. Extensive experiments validate the effectiveness of MotionWavelet, demonstrating improved prediction accuracy and enhanced generalization across various benchmarks. Our code and models will be released upon acceptance.</p>
				<br>
				<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_main_res.png" style="width:90%; margin-bottom:8px">
					</div>
					<p>
  Qualitative comparisons. The upper part shows predictions for Human3.6M, and the bottom part for HumanEva-I. The first row in each part represents ground truth motion. The closer to the ground truth motion indicates better prediction.					</p>
				</div>
				<br>
				<br>
				<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_main_res_tab.png" style="width:86%; margin-bottom:8px">
					</div>
					<p>
				Quantitative comparison between our approach and state-of-the-art methods on the HumanEva-I and Human3.6M datasets. Our method consistently demonstrates superior accuracy while maintaining commendable diversity metrics. Bold values indicate the best performance, while underlined values indicate the second best.
				</div>



			</div>
			<br>


			<div class="section abstract">
				<h2>Framework</h2>
								<br>
				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_pipeline.jpg" style="width:90%; margin-bottom:20px">
					</div>

					</div>
				</center>
			<p>
				System overview. Our method first converts motion from spatial space to Wavelet manifold and then conducts Wavelet Manifold Diffusion given few history frames where a denoiser $\epsilon_\theta$ is trained from the diffusion process $q (\mathbf{y}^{(t)}| \mathbf{y}^{(t-1)})$. During inference, the Wavelet Manifold Diffusion model predicts the latent $\mathbf{y}^{(0)}$ from condition inputs and then uses iDWT to transform it to the motion space efficiently.

			</p>
			</div>
			<br>

<!--	-->




			</div>
			<div class="section abstract">
				<h2>More Qualitative Results</h2><br>
				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_more_vis.png" style="width:88%; margin-bottom:20px">
					</div>
						<p>
							More qualitative results of MotionWavelet, where the green-purple skeletons represent the observed motions, and the red-black skeletons represent the predicted motions. We visualize 10 predicted samples. Our method produces high-fidelity and diverse motion prediction results.
						</p></div>
				</center>
				<br>
				<br>
				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_more_vis_no_overlay.png" style="width:90%; margin-bottom:20px">
					</div>
						<p>More qualitative results of MotionWavelet, where the green-purple skeletons represent the observed motions, the blue-purple skeletons represent the GT motions, and the red-black skeletons represent the predicted motions. We visualize 10 predicted samples without overlay.</p>
					</div>
				</center>
				<br>

			</div>

		<div class="section abstract">
				<h2>Controllable Human Motion Prediction</h2><br>
			<h2>Joint-level Control</h2><br>

				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_joint_level.png" style="width:88%; margin-bottom:20px">
					</div>
						<p>
							Visualizations showcasing the joint-level control motion prediction results of MotionWavelet. The green-purple skeletons represent the observed joint motions, while the red-black skeletons represent the predicted joint motions. The controlled joints are highlighted in yellow for clarity.
				</p>

				</center>
				<br>

						<h2>Motion Switch Control</h2><br>

			<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_motion_trans1.png" style="width:84%; margin-bottom:20px">
					</div>
						<p>
							Controllable Motion Prediction: Motion Switching. Visualizations showcasing the motion transfer results of MotionWavelet. The green-purple skeletons represent the observed motions, the red-black skeletons represent the predicted motions, and the blue-yellow skeletons represent the target motions		</p>

				</center>

			<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_motion_trans2.png" style="width:82%; margin-bottom:20px">
					</div>
						<p>
							Controllable Motion Prediction: Motion Switching. Visualizations showcasing the motion transfer results of MotionWavelet. The green-purple skeletons represent the observed motions, the red-black skeletons represent the predicted motions, and the blue-yellow skeletons represent the target motions		</p>

				</center>



		</div>


		<div class="section abstract">





			<br>

			<center>
					<br>
									<h2 align="center">Check out our paper for more details.</h3>
				</center>
			</div>
			<div class="section list">
				<h2>Citation</h2>
				<div class="section bibtex">
					<pre>@article{feng2024motionwavelet,
  title={MotionWavelet: Human Motion Prediction via Wavelet Manifold Learning},
  author={Yuming Feng*, Zhiyang Dou*, Ling-Hao Chen, Yuan Liu, Tianyu Li, Jingbo Wang, Zeyu Cao, Wenping Wang, Taku Komura, Lingjie Liu},
  journal={arXiv preprint},
  year={2024}
}</pre>
				</div>
			</div>
			
		<!-- 	<div class="section list">
				<h2>Related Links</h2>
				<div class="row" style="margin-top:15px">
				<li>Parts of <a href="https://github.com/Totoro97/NeuS" target="_blank">our PyTorch implementation</a> are taken from <a href="https://github.com/lioryariv/idr" target="_blank">IDR</a> and <a href="https://github.com/yenchenlin/nerf-pytorch" target="_blank">NeRF-pytorch</a>.
				<li>Check the concurrent works of learning neural implicit surfaces: </br> 			
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://arxiv.org/abs/2104.10078" target="_blank">UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction</a>, Oechsle et al. 2021 </br>
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://arxiv.org/abs/2106.12052" target="_blank">Volume Rendering of Neural Implicit Surfaces</a>, Yariv et al. 2021 
				<li>Also check other works about neural scene representations and neural rendering from our group: </br> 
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://lingjie0206.github.io/papers/NSVF/" target="_blank">Neural Sparse Voxel Fields:</a>, Liu et al. 2020 </br> 	
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/" target="_blank">Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video</a>, Tretschk et al. 2021</br> 	
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="http://gvv.mpi-inf.mpg.de/projects/NeuralActor/" target="_blank">Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control</a>, Liu et al. 2021 </br> 	
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://liuyuan-pal.github.io/NeuRay/" target="_blank">Neural Rays for Occlusion-aware Image-based Rendering</a>, Liu et al. 2021. </br> 	
				</div>
			</div>
			
			<div class="section list">
				<h2>Acknowledgements</h2>
				<div class="row" style="margin-top:15px">
				<p>We thank Michael Oechsle for providing the results of UNISURF. Christian Theobalt was supported by ERC Consolidator Grant 770784. Lingjie Liu was supported by Lise Meitner Postdoctoral Fellowship.</p> 
				</div>
			</div> -->
			
			
			<div class="section">
				<hr class="smooth">
				This page is <a href="http://www.zotero.org" target="_blank">Zotero</a> translator friendly. Page last updated 
				<script type="text/javascript">
					var m = "This page was last updated: " + document.lastModified;
					var p = m.length - 9;
					document.writeln("<left>");
					document.write(m.substring(p, 0) + ".");
					document.writeln("</left>");
				</script>
				<a href="https://www.mpi-inf.mpg.de/imprint/">Imprint</a>. <a href="https://data-protection.mpi-klsb.mpg.de/inf/gvv.mpi-inf.mpg.de/projects/">Data Protection</a>.
			</div>
		</div>
	</div>
</body>
</html>