<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<title>CBIL: Collective Behavior Imitation Learning for Fish from Real Videos</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


	<!--
	Yifan Wu* [1,2],
Zhiyang Dou* [1,3],
Yuko Ishiwaka [4],
Shun Ogawa [4],
Yuke Lou [1],
Wenping Wang [5],
Lingjie Liu [3],
Taku Komura [1].

(* Equal Contribution)

	-->
	<!-- Meta tags for Zotero grab citation -->


	<meta name="citation_title" content="CBIL: Collective Behavior Imitation Learning for Fish from Real Videos">
	<meta name="citation_author" content="Yifan Wu*">
	<meta name="citation_author" content="Zhiyang Dou*">
	<meta name="citation_author" content="Yuko Ishiwaka">
	<meta name="citation_author" content="Shun Ogawa">
	<meta name="citation_author" content="Yuke Lou">
	<meta name="citation_author" content="Wenping Wang">
	<meta name="citation_author" content="Lingjie Liu">
	<meta name="citation_author" content="Taku Komura">
	<meta name="citation_publication_date" content="2024">
	<meta name="citation_conference_title" content="SIGA 2024">
	<meta name="citation_pdf_url" content="">

	<meta name="robots" content="index,follow">
	<meta name="description"
		content="
		Reproducing realistic collective behaviors presents a captivating yet formidable challenge. Traditional rule-based methods rely on hand-crafted principles, limiting motion diversity and realism in generated collective behaviors. Recent imitation learning methods learn from data but often require ground truth motion trajectories and struggle with authenticity, especially in high-density groups with erratic movements. In this paper, we present a scalable approach, Collective Behavior Imitation Learning~(CBIL), for learning fish schooling behavior directly from videos, without relying on captured motion trajectories. Our method first leverages Video Representation Learning, where a Masked Video AutoEncoder~(MVAE) extracts implicit states from video inputs in a self-supervised manner. The MVAE effectively maps 2D observations to implicit states that are compact and expressive for following the imitation learning stage. Then, we propose a novel adversarial imitation learning method to effectively capture complex movements of the schools of fish, allowing for efficient imitation of the distribution for motion patterns measured in the latent space. It also incorporates bio-inspired rewards alongside priors to regularize and stabilize training. Once trained, \name can be used for various animation tasks with the learned collective motion priors. We further show its effectiveness across different species. Finally, we demonstrate the application of our system in detecting abnormal fish behavior from in-the-wild videos.">
	<link rel="author" href="" />


	<!-- Fonts and stuff -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800'
		rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
	<link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
	<script src="js/google-code-prettify/prettify.js"></script>



	<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
});
</script>



</head>


<body>
	<div id="content">
		<div id="content-inner">
			<div class="section logos" style="text-align:center">
				<a href="https://www.hku.hk/" target="_blank"><IMG src="./logos/Logo_HKU.png" height="67" border="0"></a></td>
<!--				<a href="https://www.bu.edu/" target="_blank"><IMG src="./logos/BU.png" height="65"-->
<!--						border="0"></a></td>-->
				<a href="https://www.upenn.edu/" target="_blank"><IMG src="./logos/penn.png" height="64"
						border="0"></a></td>
				<a href="https://www.softbank.jp/en/" target="_blank"><IMG src="./logos/SB.png" height="64"
						border="0"></a></td>
				<a href="https://www.tamu.edu/" target="_blank"><IMG src="./logos/Logo_TAMU.png" height="66" border="4"></a></td>
			</div>

			<div class="section head">

				<h1>CBIL: Collective Behavior Imitation Learning for Fish from Real Videos</h1>


	<!--
	Yifan Wu* [1,2],
Zhiyang Dou* [1,3],
Yuko Ishiwaka [4],
Shun Ogawa [4],
Yuke Lou [1],
Wenping Wang [5],
Lingjie Liu [3],
Taku Komura [1].

(* Equal Contribution)

	-->
<span style="display: block; margin-bottom: 2.4%;"></span>

			<div class="authors">
				<a href="https://www.linkedin.com/in/yifan-wu-37a58a258" target="_blank">Yifan Wu*</a><sup> 1</sup>&#160;&#160;
				<a href="https://frank-zy-dou.github.io/" target="_blank">Zhiyang Dou*</a><sup> 1,2</sup>&#160;&#160;
				<a href="https://scholar.google.com/citations?user=ECBWZwIAAAAJ&hl=ja" target="_blank">Yuko Ishiwaka</a><sup> 3</sup>&#160;&#160;
				<a href="https://scholar.google.it/citations?user=ZNKi4b4AAAAJ&hl=en" target="_blank">Shun Ogawa</a><sup> 3</sup>&#160;&#160;
				<a href="https://thorin666.github.io/" target="_blank">Yuke Lou</a><sup> 1</sup>&#160;&#160; <br><span style="display: block; margin-bottom: 0.75%;"></span>
				<a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank">Wenping Wang</a><sup> 4</sup>&#160;&#160;
				<a href="https://lingjie0206.github.io/" target="_blank">Lingjie Liu</a><sup> 2</sup>&#160;&#160;
				<a href="https://i.cs.hku.hk/~taku/" target="_blank">Taku Komura</a><sup> 1</sup>&#160;&#160;
			</div>
<span style="display: block; margin-bottom: 2%;"></span>

				<div class="affiliations">
					<sup>1</sup><a href="https://www.hku.hk/" target="_blank">The University of Hong Kong</a>&#160;&#160;
<!--					<sup>2</sup><a href="https://www.bu.edu/" target="_blank">Boston University</a>&#160;&#160;-->
					<sup>2</sup><a href="https://www.upenn.edu/" target="_blank">University of Pennsylvania</a>&#160;&#160;<br>
					<sup>3</sup><a href="https://www.softbank.jp/en/" target="_blank">SoftBank</a>&#160;&#160;
					<sup>4</sup><a href="https://www.tamu.edu/" target="_blank">Texas A&M University</a>&#160;&#160;
				</div>
				<div class="venue">SIGGRAPH ASIA 2024.<span style="display: block; margin-bottom: 0.5%;"></span>
					ACM Transactions on Graphics.</div>
				<div class="venue"> * Equal Contribution.</div>
			</div>
						<div class="section downloads">
					<center>
<!--						<br>-->
<!--						<br>-->
						<ul style="padding-left: 0">
							<li class="grid">
								<div class="griditem">
									<a href="https://dl.acm.org/doi/10.1145/3687904"><img src="images/pdf.png"></a><br/>
									<a href="https://dl.acm.org/doi/10.1145/3687904">Paper</a>
								</div>
<!--							</li><li class="grid">-->
<!--								<div class="griditem">-->
<!--									<a href=""><img src="images/video.png"></a><br/>-->
<!--									<a href="">Video</a>-->
<!--								</div>-->
<!--							</li>-->
<!--							<li class="grid">-->
<!--								<div class="griditem">-->
<!--									<a href="https://github.com/Frank-ZY-Dou/CASE"><img src="images/data_ico.png"></a><br/>-->
<!--									<a href="https://github.com/Frank-ZY-Dou/CASE">Code</a>-->

<!--								</div>-->
<!--							</li>-->
						</ul>
												<h3><strong>Also Under Patent Review Process</strong></h3>


					</center>
				</div>

<!--			<div class="section abstract">-->
<!--					<center>-->
<!--						<strong>Please check out our <a href="https://youtu.be/">video</a> for more details.</strong>-->
<!--					</center>-->
<!--			</div>-->

			<div class="section abstract">
				<h2>Abstract</h2><br>
				<div class="row" style="margin-bottom:20px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/Teaser.png" style="width:100%; margin-bottom:5px">
					</div>
					<p>CBIL learns diverse collective behaviors of simulated fish from video inputs directly, enabling real-time synthesis of diverse collective motions. (a) reference video clips; (b) simulating varied behaviors of fish schools such as circling, alignment, cohesion, and aggregation; (c) fish schools responding to external changes and interactions; (d) motion control across different species, e.g., birds.</p>
				</div>
				<p>
Reproducing realistic collective behaviors presents a captivating yet formidable challenge. Traditional rule-based methods rely on hand-crafted principles, limiting motion diversity and realism in generated collective behaviors. Recent imitation learning methods learn from data but often require ground truth motion trajectories and struggle with authenticity, especially in high-density groups with erratic movements. In this paper, we present a scalable approach, Collective Behavior Imitation Learning (CBIL), for learning fish schooling behavior directly from videos, without relying on captured motion trajectories. Our method first leverages Video Representation Learning, where a Masked Video AutoEncoder (MVAE) extracts implicit states from video inputs in a self-supervised manner. The MVAE effectively maps 2D observations to implicit states that are compact and expressive for following the imitation learning stage. Then, we propose a novel adversarial imitation learning method to effectively capture complex movements of the schools of fish, allowing for efficient imitation of the distribution for motion patterns measured in the latent space. It also incorporates bio-inspired rewards alongside priors to regularize and stabilize training. Once trained, CBIL can be used for various animation tasks with the learned collective motion priors. We further show its effectiveness across different species. Finally, we demonstrate the application of our system in detecting abnormal fish behavior from in-the-wild videos.
				</p>
			</div>
			<br>

			<!--<div class="section abstract">
				<h2>Full Video</h2><br>
				<center>
					<!-- <iframe width="640" height="360" src="data/video.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
					<!--<iframe width="640" height="360" src="https://www.youtube.com/embed/RFqPwH7QFEI" frameborder="0"
						allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
						allowfullscreen></iframe>-->
					<!--iframe src="./data/video.mp4" allow="autoplay; encrypted-media" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" width="560" height="315" frameborder="0"></iframe-->
					<!--<p style="font-size:11px; text-align:center">
					Download Video: <a href="data/video.mp4" target="_blank">HD</a> (MP4, 111 MB)
				</p>
				</center>
			</div>-->

			<div class="section abstract">
				<h2>Framework</h2>
				<br>
				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_pipeline.png" style="width:50%; margin-bottom:5px">
					</div>

					</div>
				</center>
			<p>An overview of CBIL. Our framework has three stages: the visual representation learning stage, the collective behavior imitation learning stage, and the crowd animation stage for various animation tasks. In the first stage, we train the MVAE that learns mappings from video inputs to latent states. The latent states are later used in our collective behavior imitation learning. In the second stage, we employ both data-driven motion prior learned from videos and bio-inspired motion prior for imitation learning. Finally, we demonstrate that CBIL is applicable to diverse fish school animations such as circling, alignment, and aggregation.</p>

			</div>
			<br>

			<div class="section abstract">
				<h2>Collective Motion Synthesis</h2>
				<br>

				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_gallery.png" style="width:75%; margin-bottom:8px">
					</div>
						<p>A gallery showcasing fish school animations reproduced using our method, highlighting its effectiveness across various fish species for reproducing diverse behaviors.
						</p>
					</div>
				</center>
			</div>

			<div class="section abstract">
				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_mainResult.png" style="width:75%; margin-bottom:8px">
					</div>
						<p>
We showcase multiple animations of schools of fish with various movement patterns. (a) Tuna clockwise circling with 50, 100, and 300 fish counts; (b) Shark counterclockwise circling; (c) Emperor Angel Fish alignment with 50 fish counts; (d) Sardines aggregation pattern.
						</p>
				</center>


			</div>


			<div class="section abstract">
				<h3>More Collective Behavior Patterns</h3><br>
				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_feeding.png" style="width:55%; margin-bottom:8px">
<!--						<img class="thumbnail" src="figs/fig_chasing.png" style="width:48.9%; margin-bottom:20px">-->
					</div>
						<p>Validation of Motion Prior Effectiveness in Fish Feeding Task. Our method (bottom row), trained with video-based motion priors, produces a more realistic animation of the school of fish compared to the pure rule-based approach trained without motion priors (top row). In the simulation, food is randomly generated and disappears once the closest fish agent remains within its sensor range for 3 seconds.</p>
					</div>
				</center>
			<br>
				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
<!--						<img class="thumbnail" src="figs/fig_feeding.png" style="width:49%; margin-bottom:20px">-->
						<img class="thumbnail" src="figs/fig_chasing.png" style="width:55%; margin-bottom:8px">
					</div>
						<p>Chasing in Circles. The school of fish transitions from chasing behavior to a consistent circling pattern, with the dominant fish (in red) chasing the subordinate fish (in yellow).</p>				</div>
				</center>

			</div>


			<div class="section abstract">
				<h3>Fish Abnomral Behavior Detection</h3><br>

				Real-world data on fish behavior, particularly abnormal behavior indicative of environmental stress or disease, is not only scarce but also exceedingly difficult to capture due to the vastness and inaccessibility of aquatic environments. This motivates us to train a detection model using synthetic data that simulates abnormal behaviors, supplemented with a small amount of expert-annotated real-world data.
				<br>
				<br>
				<br>
				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_distribution_bbox.png" style="width:49%; margin-bottom:8px">
<!--						<img class="thumbnail" src="figs/fig_Ab_RS.png" style="width:48.9%; margin-bottom:20px">-->
					</div>
						<p>A total of 2901 fish are annotated in our training data, comprising synthetic and real images. We present the distribution of the bounding box locations and sizes, normalized by image size.</p></div>

				</center>
				<br>

				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
<!--						<img class="thumbnail" src="figs/fig_distribution_bbox.png" style="width:49%; margin-bottom:20px">-->
						<img class="thumbnail" src="figs/fig_Ab_RS.png" style="width:48.9%; margin-bottom:8px">
					</div>
						<p>Samples of fish abnormal behavior detection from synthetic images and real images. The yellow box denotes normal fish while the red box denotes detected abnormal behaviors.</p>
				</center>



			</div>


			<div class="section abstract">
				<h3>Generalization to Other Species</h3>
				<br>
				<center>
					<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="figs/fig_Bird.png" style="width:75%; margin-bottom:8px">
<!--						<img class="thumbnail" src="figs/fig_chasing.png" style="width:48.9%; margin-bottom:20px">-->
					</div>
						<p>CBIL allows for crowd animation for different species. Here, we demonstrate the effectiveness of our method in simulating the flock of birds. Our framework reproduces a variety of behaviors learned from the reference videos.</p>	</div>
				</center>



			</div>







			<center>
					<br>
									<h2 align="center">Check out our paper for more details.</h3>
				</center>
			</div>
			<div class="section list">
				<h2>Citation</h2>
				<div class="section bibtex">
					<pre>
				</div>
			</div>
			
		<!-- 	<div class="section list">
				<h2>Related Links</h2>
				<div class="row" style="margin-top:15px">
				<li>Parts of <a href="https://github.com/Totoro97/NeuS" target="_blank">our PyTorch implementation</a> are taken from <a href="https://github.com/lioryariv/idr" target="_blank">IDR</a> and <a href="https://github.com/yenchenlin/nerf-pytorch" target="_blank">NeRF-pytorch</a>.
				<li>Check the concurrent works of learning neural implicit surfaces: </br> 			
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://arxiv.org/abs/2104.10078" target="_blank">UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction</a>, Oechsle et al. 2021 </br>
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://arxiv.org/abs/2106.12052" target="_blank">Volume Rendering of Neural Implicit Surfaces</a>, Yariv et al. 2021 
				<li>Also check other works about neural scene representations and neural rendering from our group: </br> 
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://lingjie0206.github.io/papers/NSVF/" target="_blank">Neural Sparse Voxel Fields:</a>, Liu et al. 2020 </br> 	
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/" target="_blank">Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video</a>, Tretschk et al. 2021</br> 	
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="http://gvv.mpi-inf.mpg.de/projects/NeuralActor/" target="_blank">Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control</a>, Liu et al. 2021 </br> 	
				&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://liuyuan-pal.github.io/NeuRay/" target="_blank">Neural Rays for Occlusion-aware Image-based Rendering</a>, Liu et al. 2021. </br> 	
				</div>
			</div>
			
			<div class="section list">
				<h2>Acknowledgements</h2>
				<div class="row" style="margin-top:15px">
				<p>We thank Michael Oechsle for providing the results of UNISURF. Christian Theobalt was supported by ERC Consolidator Grant 770784. Lingjie Liu was supported by Lise Meitner Postdoctoral Fellowship.</p> 
				</div>
			</div> -->
			
			
			<div class="section">
				<hr class="smooth">
				This page is <a href="http://www.zotero.org" target="_blank">Zotero</a> translator friendly. Page last updated 
				<script type="text/javascript">
					var m = "This page was last updated: " + document.lastModified;
					var p = m.length - 9;
					document.writeln("<left>");
					document.write(m.substring(p, 0) + ".");
					document.writeln("</left>");
				</script>
				<a href="https://www.mpi-inf.mpg.de/imprint/">Imprint</a>. <a href="https://data-protection.mpi-klsb.mpg.de/inf/gvv.mpi-inf.mpg.de/projects/">Data Protection</a>.
			</div>
		</div>
	</div>
</body>
</html>